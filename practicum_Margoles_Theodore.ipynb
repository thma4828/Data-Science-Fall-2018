{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3022: Intro to Data Science - Fall 2018 Practicum \n",
    "***\n",
    "\n",
    "This practicum is due on Moodle by **11:55pm on Wednesday December 12**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  \n",
    "\n",
    "**Here are the rules:** \n",
    "\n",
    "1. All work, code and analysis, must be your own. \n",
    "1. You may use your course notes, posted lecture slides, textbooks, in-class notebooks, and homework solutions as resources.  You may also search online for answers to general knowledge questions like the form of a probability distribution function or how to perform a particular operation in Python/Pandas. \n",
    "1. This is meant to be like a coding portion of your final exam. So, the instructional team will be much less helpful than we typically are with homework. For example, we will not check answers, help debug your code, and so on.\n",
    "1. If something is left open-ended, it is because we want to see how you approach the kinds of problems you will encounter in the wild, where it will not always be clear what sort of tests/methods should be applied. Feel free to ask clarifying questions though.\n",
    "2. You may **NOT** post to message boards or other online resources asking for help.\n",
    "3. You may **NOT** copy-paste solutions *from anywhere*.\n",
    "4. You may **NOT** collaborate with classmates or anyone else.\n",
    "5. In short, **your work must be your own**. It really is that simple.\n",
    "\n",
    "Violation of the above rules will result in an immediate academic sanction (*at the very least*, you will receive a 0 on the practicum or an F in the course, depending on severity), and a trip to the Honor Code Council.\n",
    "\n",
    "**By submitting this assignment, you agree to abide by the rules given above.**\n",
    "\n",
    "***\n",
    "\n",
    "**Name**:  Theodore Margoles\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- You may not use late days on the practicum nor can you drop your practicum grade. \n",
    "- If you have a question for us, post it as a **PRIVATE** message on Piazza.  If we decide that the question is appropriate for the entire class, then we will add it to a Practicum clarifications thread. \n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. \n",
    "- This should go without saying, but... For any question that asks you to calculate something, you **must show all work to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit.\n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1](#p1) | [Problem 2](#p2) | [Problem 3](#p3)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np \n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p1'></a>\n",
    "\n",
    "### [35 points] Problem 1: Yahtzee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A:** You are playing [Yahtzee](https://en.wikipedia.org/wiki/Yahtzee) with your friends. A player's turn in Yahtzee consists of rolling a set of 5 dice. Then the player is given two additional rolls, where they are allowed to re-roll any number of the dice, including potentially all of them or none of them. The goal is to obtain certain combinations of the dice values resulting after the third roll. Different combinations are worth different amounts of points, and the goal of the game is to get as many points as possible.\n",
    "\n",
    "This game of Yahtzee is a bit unlike any you have ever played before, however. This is because Darth Ketelsen is back, and with her she brought her famous **5-sided dice**. These are fair dice with sides numbered 1-5. So, you are playing Yahtzee with a Sith Lord with 5-sided dice. Indeed, things just got real.\n",
    "\n",
    "A **straight** in Darth Ketelsen's game consists of 5 values all in a row. For example, the outcome $[1,2,3,4,5]$ is a  straight but the outcome $[1,2,3,4,4]$ is not.\n",
    "\n",
    "**Do two things:**\n",
    "1. Compute by hand the probability of rolling a straight in a single roll of all 5 dice. Show all work.\n",
    "2. Write a simulation to verify the probability that you computed. Run at least 10,000 simulations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rolling 5 dice at once is equivalent to taking 5 bernouli trials, so we can think about just rolling a dice 5 times and think about the probability of this being a straight.\n",
    "\n",
    "\n",
    "The sample space of 5 rolls consists of a total of $5^5 =  3125$ possible outcomes\n",
    "\n",
    "the event consists of {[1,2,3,4,5] ,  [5,4,3,2,1],} which is only 2 possible outcomes...\n",
    "\n",
    "so the probability of rolling a straight then is: $\\frac{2}{3125} = 0.00064$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated probability of getting a straight with 5 sided dice:  0.0007\n",
      "error from calculated probability:  5.999999999999994e-05\n"
     ]
    }
   ],
   "source": [
    "def is_A_Straight(five_rolls): #definately finds all the straights!\n",
    "    lastNum = five_rolls[0]\n",
    "    AscendingCount = 0\n",
    "    for index, item in iter(enumerate(five_rolls)): #this loop finds ascending straights (iter enumerate for the win!)\n",
    "        if index != 0:\n",
    "            if item == lastNum + 1:\n",
    "                AscendingCount += 1     \n",
    "        lastNum = item\n",
    "    lastNum = five_rolls[0]\n",
    "    DescendingCount = 0\n",
    "    for index, item in iter(enumerate(five_rolls)): #this loop finds descending straights\n",
    "        if index != 0:\n",
    "            if item == lastNum -1:\n",
    "                DescendingCount += 1\n",
    "        lastNum = item\n",
    "    if AscendingCount == 4 or DescendingCount == 4: #due to zero base indexing\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "                \n",
    "\n",
    "def yahtzee_simulator(numtrials):\n",
    "    rolls_data = []\n",
    "    die = [1,2,3,4,5]\n",
    "    for _ in range(1, numtrials + 1):\n",
    "        five_rolls = []\n",
    "        for __ in range(1, 6):\n",
    "            five_rolls.append(np.random.choice(die, size=1, replace=True)) #make a die roll 5 times \n",
    "        val = is_A_Straight(five_rolls)\n",
    "        rolls_data.append(val)\n",
    "    return np.mean(rolls_data) #return the means of 0s and 1s which gives us the probability as estimated by numtrials trials\n",
    "\n",
    "meanval = yahtzee_simulator(100000)\n",
    "print(\"estimated probability of getting a straight with 5 sided dice: \", meanval)\n",
    "print(\"error from calculated probability: \", np.abs(meanval - 0.00064))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B:** The goal of this problem is to compute the probability of getting a straight using all three of your rolls, instead of just the single roll approach that you computed in Part A. Here, we'll need to implement a strategy so that after the first roll and after the second roll, we keep the dice that get us closer to a straight and re-roll the dice that are not useful for our straight.\n",
    "\n",
    "For instance, suppose your first roll comes up $[1,2,3,3,3]$. You really want to get that straight! So, you would follow the strategy of saving the $[1,2,3]$ and re-roll two of the threes, hoping for a 4 and 5 to get the straight. Then, for your third roll, you would save as many of the dice as possible that would be part of a straight, and re-roll any remaining dice.\n",
    "\n",
    "Finish the function below called `dire_straights` to simulate many complete 3-roll turns, and computes the probability of ending your turn with a straight. The only input to the function should be `ntrial`, an integer for the number of turns to simulate. Remember, each turn consists of 3 rolls.\n",
    "\n",
    "Then, use your function to estimate the probability of a straight after a full turn of Yahtzee. Use at least 10,000 simulations, and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after a full 3 rolls of yahtzee the odds of getting a straight are:  0.4586\n"
     ]
    }
   ],
   "source": [
    " def retain_best_first_cards(hand):\n",
    "    newhand = []\n",
    "    ascending_indices = []\n",
    "    for index, item in iter(enumerate(hand)):\n",
    "        if index != 0:\n",
    "            if item == hand[index-1] + 1 :\n",
    "                ascending_indices.append(index)\n",
    "        else:\n",
    "            if item == hand[index+1] - 1:\n",
    "                ascending_indices.append(index)\n",
    "    descending_indices = []             \n",
    "    for index, item in iter(enumerate(hand)):\n",
    "        if index != 0:\n",
    "            if item == hand[index-1] - 1:\n",
    "                descending_indices.append(index)\n",
    "        else:\n",
    "            if item == hand[index+1] + 1:\n",
    "                descending_indices.append(index)\n",
    "    if len(ascending_indices) >= len(descending_indices):            \n",
    "        for i in ascending_indices:\n",
    "            newhand.append(hand[i])\n",
    "    else:\n",
    "        for j in descending_indices:\n",
    "            newhand.append(hand[j])\n",
    "        \n",
    "    return newhand\n",
    "            \n",
    " \n",
    " def dire_straights(ntrial):\n",
    "    die = [1,2,3,4,5]\n",
    "    results = []\n",
    "    for trial in range(ntrial):\n",
    "        hand = []\n",
    "        first_draw = np.random.choice(die, size=5, replace=True)\n",
    "        best_straight = retain_best_first_cards(first_draw)\n",
    "        turn = 0\n",
    "        gotStraight = False\n",
    "        while turn <= 2 and not gotStraight:\n",
    "            if len(best_straight) == 5:\n",
    "                gotStraight = True\n",
    "            else:\n",
    "                newcards = np.random.choice(die, size=5, replace=True)\n",
    "                if len(best_straight) > 0:\n",
    "                    upper = max(best_straight)\n",
    "                else:\n",
    "                    upper = 0\n",
    "                appendthese = []\n",
    "                for card in newcards:\n",
    "                    if card > upper and len(best_straight) < 5:\n",
    "                        best_straight.append(card)\n",
    "                if is_A_Straight(best_straight):\n",
    "                    gotStraight = True\n",
    "            turn += 1\n",
    "        if gotStraight is True:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "    return results\n",
    "            \n",
    "            \n",
    "print(\"after a full 3 rolls of yahtzee the odds of getting a straight are: \", np.mean(dire_straights(10000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**my resuls are that there is about a 46% chance of getting a straight in your three rolls, \n",
    "this is actually slightly lower than my intuition would lead me to expect so it makes sense in the context of being a cleverly designed game! However I also wonder if my simulation is missing any cases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C:** Write a simulation to estimate the probability of obtaining a straight if the first roll contains exactly three distinct unique values. For example, a valid first roll could be $[1,5,3,3,3]$ but not $[1,3,3,4,5]$. You are still using the set of 5-sided dice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After $10,000$ iterations my simulation predicts:  $0.447444352844188$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated prob:  0.447444352844188\n"
     ]
    }
   ],
   "source": [
    "def is_only_three(hand):\n",
    "    hashTable = {}\n",
    "    hashTable[1] = 0\n",
    "    hashTable[2] = 0\n",
    "    hashTable[3] = 0\n",
    "    hashTable[4] = 0\n",
    "    hashTable[5] = 0\n",
    "    \n",
    "    for card in hand:\n",
    "        hashTable[card] += 1\n",
    "    \n",
    "    ucount = 0\n",
    "    for key in hashTable:\n",
    "        if hashTable[key] > 0:\n",
    "            ucount += 1\n",
    "        \n",
    "    if ucount == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def three_on_first(ntrials):\n",
    "    die = [1,2,3,4,5]\n",
    "    results_three_unique = []\n",
    "    for trial in range(ntrials):\n",
    "        isThree = False\n",
    "        hand = []\n",
    "        first_draw = np.random.choice(die, size=5, replace=True)\n",
    "        if is_only_three(first_draw) == 1:\n",
    "            isThree = True\n",
    "        best_straight = retain_best_first_cards(first_draw)\n",
    "        turn = 0\n",
    "        gotStraight = False\n",
    "        while turn < 2 and not gotStraight:\n",
    "            if len(best_straight) == 5:\n",
    "                gotStraight = True\n",
    "            else:\n",
    "                newcards = []\n",
    "                for p in range(5):\n",
    "                    newcards.append(np.random.choice(die, size=1, replace=True))\n",
    "                if len(best_straight) > 0:\n",
    "                    upper = max(best_straight)\n",
    "                else:\n",
    "                    upper = 0\n",
    "                appendthese = []\n",
    "                for card in newcards:\n",
    "                    if card > upper and len(best_straight) < 5:\n",
    "                        best_straight.append(card)\n",
    "                if is_A_Straight(best_straight):\n",
    "                    gotStraight = True\n",
    "            turn += 1\n",
    "        if isThree is True and gotStraight is True:\n",
    "            results_three_unique.append(1)\n",
    "        elif isThree is True and gotStraight is False:\n",
    "            results_three_unique.append(0)\n",
    "    return np.mean(results_three_unique)\n",
    "\n",
    "            \n",
    "            \n",
    "print(\"estimated prob: \", three_on_first(10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D:** Verify your calculation from Part C by hand. Show all work, and comment on whether the two agree.\n",
    "\n",
    "*Hint: you will need to consider a variety of different cases - what are all the ways you could end up with a straight, given that your first roll contained exactly 3 unique values?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by considering how many ways you could end up with 3 unique values in your hand of 5 cards.\n",
    "\n",
    "This should be $5\\choose{3}$ = $\\frac{5!}{(3!)(2!)}$ because this is the number of ways we can select 3 values from 5. \n",
    "\n",
    "$\\frac{5!}{(3!)(2!)}$ = $10$\n",
    "\n",
    "now we need to consider for each of these cases how many ways there are to then get a straight.\n",
    "\n",
    "If we have 3 unique values out of 5 this implies that over the next 2 rolls, we must only pick up 2 unique values from the remaining 10 cards (2 draws) we have to get a straight. \n",
    "\n",
    "There are therefore $10\\choose{2}$ = $45$ ways we could get 2 cards out of the remaining 10, and only 2 of these cases should satisfy the rest of the straight, \n",
    "\n",
    "so we have $\\frac{2}{45}$ x $\\frac{10}{1}$ = $\\frac{20}{45}$ = $0.4444$\n",
    "\n",
    "yes my calculation is very very close to the simulation! \n",
    "\n",
    "the simulated value: $0.447$ and the the actual value: $0.4444$\n",
    "\n",
    "which is close enough for me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0\n",
      "0.0016999999999999793\n"
     ]
    }
   ],
   "source": [
    "def factorial(n):\n",
    "    if n == 1 or n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n*factorial(n-1)\n",
    "\n",
    "def n_choose_r(n, r):\n",
    "    return (factorial(n))/(factorial(r)*factorial(n-r))\n",
    "\n",
    "print(n_choose_r(10, 2))\n",
    "print(0.4461-0.4444)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E:**  Your friend offers you the following deal. Each time your Yahtzee turn (i.e., all three rolls) results in a 5-of-a-kind, she will give you \\$5. Each time your Yahtzee turn results in a straight, she will give you \\$3. But, she will charge you \\$1 for each turn (where a turn includes all 3 rolls of the five 5-sided dice). Should you take this deal? Fully justify your answer using calculations that include expected values. You may include some simulations to estimate relevant probabilities. Clearly state any assumptions you are making in your modeling choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**each time I roll either a straight of 5 of a kind I will have a straight so hypothetically the upper bound payout I will recieve is 3\\$. Further the calculated probability of such an event is 0.44 so that would be a 44% chance of recieving 3\\$ and a 100% chance of paying out 1\\$. **\n",
    "\n",
    "**the assumption here is that my calculated probability of 0.44 is the correct ratio of times that you would get a straight. Something in my intution still thinks that this number should be higher, however that would mean it would be even more profitable that my simulation, therefore I think I am safe in saying that I will take this deal. **\n",
    "\n",
    "**my python simulations seam to solidly support my conclusion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 100 turns playing your net money gained is:  29\n",
      "After 1100 turns playing your net money gained is:  316\n",
      "After 11100 turns playing your net money gained is:  3204\n"
     ]
    }
   ],
   "source": [
    "ps = [0.56, 0.44]\n",
    "choice = [\"no straight\", \"straight\"]\n",
    "\n",
    "netMoney = 0\n",
    "for i in range(100):\n",
    "    c = np.random.choice(choice,replace=True,p=ps,size=1)\n",
    "    if c == \"straight\":\n",
    "        netMoney += 3\n",
    "    netMoney -= 1\n",
    "print(\"After 100 turns playing your net money gained is: \", netMoney)\n",
    "    \n",
    "for i in range(1000):\n",
    "    c = np.random.choice(choice,replace=True,p=ps,size=1)\n",
    "    if c == \"straight\":\n",
    "        netMoney += 3\n",
    "    netMoney -= 1    \n",
    "    \n",
    "print(\"After 1100 turns playing your net money gained is: \", netMoney)\n",
    "    \n",
    "for i in range(10000):\n",
    "    c = np.random.choice(choice,replace=True,p=ps,size=1)\n",
    "    if c == \"straight\":\n",
    "        netMoney += 3\n",
    "    netMoney -= 1 \n",
    "    \n",
    "print(\"After 11100 turns playing your net money gained is: \", netMoney)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p2'></a>\n",
    "\n",
    "### [30 points] Problem 2: Sharknado Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Governor Hickenlooper has charged you with the task of assessing the factors associated with sharknado risk in Colorado. As everyone knows, sharknadoes are a leading cause of sharknado-related illness, and you are a world-renowned data/shark scientist.\n",
    "\n",
    "You decide to use multiple linear regression to understand and predict what factors lead to increased sharknado hazard. Your lead scientist, aptly named Fin, has collected lots of relevant data at a local sharknado hotspot, the Boulder Reservoir[\\*](#footnote). The data cover a variety of sharknado-related environmental and other conditions, and you'll find this data in the file `sharknadoes.csv`. \n",
    "\n",
    "**Response**: \n",
    "\n",
    "- $\\texttt{sharknado hazard}$: the hazard of a sharknado, where 1 is very unlikely and 100 is highly likely\n",
    "\n",
    "**Features**: \n",
    "\n",
    "- $\\texttt{taunts}$: the number of times over the past year that someone has taunted a shark\n",
    "- $\\texttt{clouds}$: what percentage of the sky was covered by clouds (fraction, 0-1)\n",
    "- $\\texttt{precipitation}$: amount of precipitation in the past 72 hours (inches)\n",
    "- $\\texttt{earthquake}$: the intensity of the most recent earthquake measured in the continental United States\n",
    "- $\\texttt{shark attacks}$: the number of shark attacks within 72 hours prior to the observation\n",
    "- $\\texttt{ice cream sold}$: the number of units of ice cream sold at the beach concession stand \n",
    "- $\\texttt{misery index}$: an economic indicator for how miserable the average United States citizen is, based on the unemployment rate and the inflation rate. More [here](https://www.stuffyoushouldknow.com/podcasts/whats-the-misery-index.htm) and [here](https://en.wikipedia.org/wiki/Misery_index_(economics)). Higher values correspond to more miserable citizens.\n",
    "- $\\texttt{temperature}$: the outside temperature, measured in degrees Fahrenheit\n",
    "- $\\texttt{humidity}$: relative humidity (percent, 0-100)\n",
    "- $\\texttt{pizzas sold}$: the number of pizzas sold at the beach concession stand in the past year\n",
    "- $\\texttt{pressure}$: local air pressure (millibar) \n",
    "- $\\texttt{octopuses}$: the number of octupuses in the vicinity on the day of the observation\n",
    "- $\\texttt{Dan's shoe size}$: the size of the shoes Dan was wearing when the observation was made\n",
    "- $\\texttt{Tony's shoe size}$: the size of the shoes Tony was wearing when the observation was made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Read the data from `sharknadoes.csv` into a Pandas DataFrame.  Note that since we will be doing a multiple linear regression we will need all of the features, so you should drop any row in the DataFrame that is missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clouds</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>pizzas sold</th>\n",
       "      <th>taunts</th>\n",
       "      <th>pressure</th>\n",
       "      <th>shark attacks</th>\n",
       "      <th>octopuses</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>misery index</th>\n",
       "      <th>ice cream sold</th>\n",
       "      <th>humidity</th>\n",
       "      <th>temperature</th>\n",
       "      <th>Dans shoe size</th>\n",
       "      <th>Tonys shoe size</th>\n",
       "      <th>sharknado hazard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>7.1</td>\n",
       "      <td>5560.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>847.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.824059</td>\n",
       "      <td>12.987180</td>\n",
       "      <td>273.0</td>\n",
       "      <td>86.41</td>\n",
       "      <td>78.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>40.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>7.4</td>\n",
       "      <td>5179.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>844.34</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.993296</td>\n",
       "      <td>16.765435</td>\n",
       "      <td>184.0</td>\n",
       "      <td>96.67</td>\n",
       "      <td>89.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>36.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5227.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>839.48</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.173342</td>\n",
       "      <td>16.494518</td>\n",
       "      <td>141.0</td>\n",
       "      <td>53.85</td>\n",
       "      <td>65.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.13</td>\n",
       "      <td>7.9</td>\n",
       "      <td>5226.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>851.28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.919291</td>\n",
       "      <td>8.277176</td>\n",
       "      <td>146.0</td>\n",
       "      <td>88.72</td>\n",
       "      <td>36.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>85.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>7.5</td>\n",
       "      <td>5491.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>852.67</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.729127</td>\n",
       "      <td>5.904750</td>\n",
       "      <td>178.0</td>\n",
       "      <td>63.08</td>\n",
       "      <td>72.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>56.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clouds  earthquake  pizzas sold  taunts  pressure  shark attacks  \\\n",
       "0    1.00         7.1       5560.0    15.0    847.12            2.0   \n",
       "1    1.00         7.4       5179.0    20.0    844.34            4.0   \n",
       "2    1.00         7.0       5227.0     0.0    839.48            9.0   \n",
       "3    0.13         7.9       5226.0    34.0    851.28            2.0   \n",
       "4    1.00         7.5       5491.0     6.0    852.67            2.0   \n",
       "\n",
       "   octopuses  precipitation  misery index  ice cream sold  humidity  \\\n",
       "0        7.0       0.824059     12.987180           273.0     86.41   \n",
       "1        5.0       0.993296     16.765435           184.0     96.67   \n",
       "2        2.0       1.173342     16.494518           141.0     53.85   \n",
       "3        6.0       0.919291      8.277176           146.0     88.72   \n",
       "4        4.0       1.729127      5.904750           178.0     63.08   \n",
       "\n",
       "   temperature  Dans shoe size  Tonys shoe size  sharknado hazard  \n",
       "0         78.0            42.0              9.0             40.22  \n",
       "1         89.0            42.0              9.5             36.42  \n",
       "2         65.0             9.5              9.0             19.54  \n",
       "3         36.0             9.5             10.0             85.00  \n",
       "4         72.0            42.0              9.0             56.34  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSharkys = pd.read_csv(\"data/sharknadoes.csv\")\n",
    "dfClean = dfSharkys.dropna(axis=0)\n",
    "Y = dfClean['sharknado hazard'] #response\n",
    "X = dfClean.iloc[:,:-1] #all features\n",
    "\n",
    "dfClean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Perform the appropriate statistical test at the $\\alpha = 0.01$ significance level to determine if _at least one_ of the features is related to the the response $y$.  Clearly describe your methodology and show all computations in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform an $F$-test for ML-regression at the $\\alpha = 0.01$ confidence level. \n",
    "\n",
    "First we recall that the F-statistic is caluclated by: \n",
    "\n",
    "$\\frac{Explained \\space Variance}{Unexplained \\space Variance}$==$\\frac{MSM}{MSE}$ \n",
    "\n",
    "where $MSM$ = $\\frac{SSM}{DFM}$ \n",
    "\n",
    "and where $MSE$ = $\\frac{SSE}{DFE}$\n",
    "\n",
    "now lets break it down to each piece:\n",
    "\n",
    "$SSM$ = Sum of squared differences between the model and the mean. \n",
    "$DFM$ = correct degrees of freedome for the model, = $p-1$ = $15-1$ = $14$\n",
    "\n",
    "$SSE$ = Sum of squared errors, this is the sum of differences between the actual data and the model. \n",
    "$DFE$ = degrees of freedome for error = $n-p$ = $72-15$ = $57$.\n",
    "\n",
    "\n",
    "so new we can setup the following equations to solve:\n",
    "\n",
    "$MSM$ = $\\frac{SSM}{14}$ \n",
    "\n",
    "$MSE$ = $\\frac{SSE}{57}$\n",
    "\n",
    "so, $F$ = $\\frac{\\frac{SSM}{14}}{\\frac{SSE}{57}}$ = 179.4 as shown by python below, \n",
    "\n",
    "this gives us a **p-value of 9.60e-42** which is well below our **alpha level of 0.01**, so we can **reject the null hypothesis\n",
    "and instead conclude that there is at least one feature that has a slope $B_{j}$ $\\ne $ $0$. **\n",
    "\n",
    "that is, at least one of the features in the feature set is relevant to the result; the hazard of sharknadoes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179.35849974932614\n",
      "9.599519434024341e-42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>sharknado hazard</td> <th>  R-squared:         </th> <td>   0.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   179.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 12 Dec 2018</td> <th>  Prob (F-statistic):</th> <td>9.60e-42</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:45:33</td>     <th>  Log-Likelihood:    </th> <td> -174.23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    72</td>      <th>  AIC:               </th> <td>   378.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    57</td>      <th>  BIC:               </th> <td>   412.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    14</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>-2549.8985</td> <td>   67.605</td> <td>  -37.718</td> <td> 0.000</td> <td>-2685.275</td> <td>-2414.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clouds</th>          <td>   -1.5106</td> <td>    2.566</td> <td>   -0.589</td> <td> 0.558</td> <td>   -6.650</td> <td>    3.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>earthquake</th>      <td>    2.5079</td> <td>    0.467</td> <td>    5.367</td> <td> 0.000</td> <td>    1.572</td> <td>    3.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pizzas sold</th>     <td>   -0.0006</td> <td>    0.002</td> <td>   -0.373</td> <td> 0.711</td> <td>   -0.004</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>taunts</th>          <td>    0.3117</td> <td>    0.042</td> <td>    7.447</td> <td> 0.000</td> <td>    0.228</td> <td>    0.396</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pressure</th>        <td>    3.0688</td> <td>    0.079</td> <td>   38.850</td> <td> 0.000</td> <td>    2.911</td> <td>    3.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shark attacks</th>   <td>   -0.1151</td> <td>    0.144</td> <td>   -0.797</td> <td> 0.429</td> <td>   -0.404</td> <td>    0.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>octopuses</th>       <td>   -0.0749</td> <td>    0.143</td> <td>   -0.524</td> <td> 0.602</td> <td>   -0.361</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>precipitation</th>   <td>    1.3982</td> <td>    0.930</td> <td>    1.503</td> <td> 0.138</td> <td>   -0.464</td> <td>    3.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>misery index</th>    <td>    0.0273</td> <td>    0.080</td> <td>    0.340</td> <td> 0.735</td> <td>   -0.133</td> <td>    0.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ice cream sold</th>  <td>    0.0096</td> <td>    0.008</td> <td>    1.193</td> <td> 0.238</td> <td>   -0.007</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>humidity</th>        <td>    0.0188</td> <td>    0.027</td> <td>    0.706</td> <td> 0.483</td> <td>   -0.035</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>temperature</th>     <td>   -0.4426</td> <td>    0.053</td> <td>   -8.396</td> <td> 0.000</td> <td>   -0.548</td> <td>   -0.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Dans shoe size</th>  <td>    0.0271</td> <td>    0.023</td> <td>    1.175</td> <td> 0.245</td> <td>   -0.019</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Tonys shoe size</th> <td>    0.2814</td> <td>    1.273</td> <td>    0.221</td> <td> 0.826</td> <td>   -2.268</td> <td>    2.831</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.821</td> <th>  Durbin-Watson:     </th> <td>   2.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.663</td> <th>  Jarque-Bera (JB):  </th> <td>   0.300</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.005</td> <th>  Prob(JB):          </th> <td>   0.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.316</td> <th>  Cond. No.          </th> <td>1.03e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.03e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:       sharknado hazard   R-squared:                       0.978\n",
       "Model:                            OLS   Adj. R-squared:                  0.972\n",
       "Method:                 Least Squares   F-statistic:                     179.4\n",
       "Date:                Wed, 12 Dec 2018   Prob (F-statistic):           9.60e-42\n",
       "Time:                        09:45:33   Log-Likelihood:                -174.23\n",
       "No. Observations:                  72   AIC:                             378.5\n",
       "Df Residuals:                      57   BIC:                             412.6\n",
       "Df Model:                          14                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const           -2549.8985     67.605    -37.718      0.000   -2685.275   -2414.522\n",
       "clouds             -1.5106      2.566     -0.589      0.558      -6.650       3.628\n",
       "earthquake          2.5079      0.467      5.367      0.000       1.572       3.444\n",
       "pizzas sold        -0.0006      0.002     -0.373      0.711      -0.004       0.003\n",
       "taunts              0.3117      0.042      7.447      0.000       0.228       0.396\n",
       "pressure            3.0688      0.079     38.850      0.000       2.911       3.227\n",
       "shark attacks      -0.1151      0.144     -0.797      0.429      -0.404       0.174\n",
       "octopuses          -0.0749      0.143     -0.524      0.602      -0.361       0.211\n",
       "precipitation       1.3982      0.930      1.503      0.138      -0.464       3.261\n",
       "misery index        0.0273      0.080      0.340      0.735      -0.133       0.188\n",
       "ice cream sold      0.0096      0.008      1.193      0.238      -0.007       0.026\n",
       "humidity            0.0188      0.027      0.706      0.483      -0.035       0.072\n",
       "temperature        -0.4426      0.053     -8.396      0.000      -0.548      -0.337\n",
       "Dans shoe size      0.0271      0.023      1.175      0.245      -0.019       0.073\n",
       "Tonys shoe size     0.2814      1.273      0.221      0.826      -2.268       2.831\n",
       "==============================================================================\n",
       "Omnibus:                        0.821   Durbin-Watson:                   2.225\n",
       "Prob(Omnibus):                  0.663   Jarque-Bera (JB):                0.300\n",
       "Skew:                           0.005   Prob(JB):                        0.861\n",
       "Kurtosis:                       3.316   Cond. No.                     1.03e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.03e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I use python to do the heavy lifting and just look at my fvalue and pvalue for the f-statistic\n",
    "x = sm.add_constant(X) \n",
    "model = sm.OLS(Y, x).fit() \n",
    "print(model.fvalue)\n",
    "print(model.f_pvalue)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Write a function `backward_select(df, resp_str, maxsse)` that takes in the DataFrame (`df`), the name of the column corresponding to the response (`resp_str`), and the maximum desired sum of squared errors (`maxsse`), and returns a list of feature names corresponding to the most important features via backward selection.  Use your code to determine the reduced MLR model with the minimal number of features such that the SSE of the reduced model is less than 570. At each stage in backward selection you should remove the feature that has the highest p-value associated with the hypothesis test for the given slope coefficient $\\beta_k \\neq 0$.\n",
    "\n",
    "Your code should clearly indicate which feature was removed in each stage, and the SSE associated with the model fit before the feature's removal. _Specifically, please write your code to print the name of the feature that is going to be removed and the SSE before its removal_. Afterward, be sure to report all of the retained features and the SSE of the reduced model.\n",
    "\n",
    "**Note**: The point of this exercise is to see if you can implement **backward_select** yourself.  You may of course use canned routines like statmodels OLS, but you may not call any Python method that explicitly performs backward selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward select is dropping the feature:  pizzas sold  from the data!!!\n",
      "backward select is dropping the feature:  ice cream sold  from the data!!!\n",
      "backward select is dropping the feature:  humidity  from the data!!!\n",
      "backward select is dropping the feature:  misery index  from the data!!!\n",
      "backward select is dropping the feature:  Dans shoe size  from the data!!!\n",
      "backward select is dropping the feature:  shark attacks  from the data!!!\n",
      "backward select is dropping the feature:  octopuses  from the data!!!\n",
      "backward select is dropping the feature:  taunts  from the data!!!\n",
      "backward select is dropping the feature:  precipitation  from the data!!!\n",
      "backward select is dropping the feature:  temperature  from the data!!!\n",
      "backward select is dropping the feature:  Tonys shoe size  from the data!!!\n",
      "backward select is dropping the feature:  pressure  from the data!!!\n",
      "oldsse:  23477.74879453735\n",
      "newsse:  5531.396369760583\n",
      "===remaining features and their slopes!===\n",
      "const         22.671217\n",
      "clouds       -19.644575\n",
      "earthquake     7.593021\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def backward_select(df, resp_str, maxsse):\n",
    "    Y = df[resp_str] #response\n",
    "    X = df.iloc[:,:-1] #all features except the response initially\n",
    "    X = sm.add_constant(X) #add the initial constant\n",
    "    model = sm.OLS(Y, X).fit() #make the initial fit\n",
    "\n",
    "    params = model.params #get the initial parameters\n",
    "    indices = params.index #get all initial variables.... etc etc\n",
    "    pvalue = model.f_pvalue\n",
    "    fvalue = model.fvalue\n",
    "    sse = oldsse = model.ess\n",
    "    numparamsleft = 14\n",
    "    while sse >= maxsse and numparamsleft > 1: #while we have not optimized enough and while we still have features to remove with backward select\n",
    "        worstSlopeSeen = max(np.abs(params))\n",
    "        worstSlopeCol = 'null'\n",
    "        for index, param in iter(enumerate(params)):\n",
    "            #print(index, param, indices[index])\n",
    "            if np.abs(param) < worstSlopeSeen and indices[index] != 'const': #find feature with the least slope\n",
    "                worstSlopeSeen = np.abs(param)\n",
    "                worstSlopeCol = indices[index]\n",
    "        print(\"backward select is dropping the feature: \", worstSlopeCol, \" from the data!!!\")\n",
    "        df = df.drop(axis=1, labels=[worstSlopeCol])\n",
    "        X = df.iloc[:,:-1] #all features except the response initially\n",
    "        X = sm.add_constant(X) \n",
    "        model = sm.OLS(Y, X).fit() #update all variables and models with new params!\n",
    "        params = model.params\n",
    "        indices = params.index\n",
    "        pvalue = model.f_pvalue\n",
    "        fvalue = model.fvalue\n",
    "        sse = model.ess\n",
    "        numparamsleft -= 1\n",
    "    print(\"oldsse: \", oldsse)\n",
    "    print(\"newsse: \", sse)\n",
    "    print(\"===remaining features and their slopes!===\")\n",
    "    print(model.params)\n",
    "    return df, model, model.params, oldsse, sse\n",
    "    \n",
    "dfCurr, modelnew, paramList, oldsse, newsse = backward_select(dfClean, \"sharknado hazard\", 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Write down the multiple linear regression model, including estimated parameters, obtained by your backward selection process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_{i} = B_0 +B_{1}X_1 + B_{2}X_2$\n",
    "\n",
    "\n",
    "$y_{i} = 22.67 -19.64X_{1i} + 7.593021X_{2i}$\n",
    "\n",
    "where $X_1$ is the cloud data and $X_2$ is the earthquake data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Perform the appropriate statistical test at the $\\alpha = 0.01$ significance level to determine whether there is a statistically significant difference between the full model with all features and the reduced model obtained by backward selection in **Part D**. You may use output from your model fit above, but all calculations should be set up in Markdown/MathJax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we want to see if we really made a difference by removing all those features from the data, \n",
    "\n",
    "so we must set up an f-statistic of the following form: \n",
    "\n",
    "$F$ = $\\frac{\\frac{(SSE_{full} - SSE_{red})}{p-k}}{\\frac{SSE_{full}}{(n-p-1)}}$\n",
    "\n",
    "\n",
    "== $F$ = $\\frac{\\frac{23477.75 - 5531.396}{p-k}}{\\frac{5531.396}{(n-p-1)}}$\n",
    "== $F$ = $\\frac{\\frac{23477.75 - 5531.396}{2}}{\\frac{5531.396}{(14)}}$ == $22.7$\n",
    "\n",
    "so $p-k$ represents the degrees of freedom of reduced, and n-p-1 is the degrees of freedom of the full so \n",
    "we have \n",
    "$p-k$ = 2\n",
    "$n-p-1$ = 14\n",
    "\n",
    "Now, the pvalue from the f-test is:\n",
    "$1$ - $stats.f.cdf(22.7, 2, 14)$ = $4.040090883750125e-05$ which is below our alpha level of $0.01$, \n",
    "\n",
    "therefore we can reject the null hypothesis and conclude that there is a significant difference between the $SSE_{full}$ and the $SSE_{red}$ and therefore a statistically significant difference between the full and reduced model, namely that the reduced model is actually better since the Sum of Squared Error is lower. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSEfull:  23477.74879453735 \n",
      " SSEred:  5531.396369760583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.040090883750125e-05"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"SSEfull: \", oldsse, \"\\n SSEred: \", newsse)\n",
    "model.summary()\n",
    "\n",
    "1 - stats.f.cdf(22.7, 2, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**: Based on your conclusions in **Part E**, use the _better_ of the two models to predict the sharknado hazard when the following features are observed: \n",
    "\n",
    "- $\\texttt{taunts}$: 47\n",
    "- $\\texttt{clouds}$: 0.8\n",
    "- $\\texttt{precipitation}$: 1 inch\n",
    "- $\\texttt{earthquake}$: 5\n",
    "- $\\texttt{shark attacks}$: 11\n",
    "- $\\texttt{ice cream sold}$: 120\n",
    "- $\\texttt{misery index}$: 15\n",
    "- $\\texttt{temperature}$: 70 degrees F\n",
    "- $\\texttt{humidity}$: 83\n",
    "- $\\texttt{pizzas sold}$: 5500\n",
    "- $\\texttt{pressure}$: 850 millibar \n",
    "- $\\texttt{octopuses}$: 6\n",
    "- $\\texttt{Dan's shoe size}$: 9.5\n",
    "- $\\texttt{Tony's shoe size}$: 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression model predicts:  44.9231\n"
     ]
    }
   ],
   "source": [
    "clouds = 0.8\n",
    "earthquake = 5\n",
    "\n",
    "print(\"linear regression model predicts: \", 22.67 - (19.64*0.8) + (7.59302*5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The linear Regression Model is:\n",
    "\n",
    "$y_{i} = 22.67 -19.64X_{1i} + 7.593021X_{2i}$\n",
    "\n",
    "where $X_1$ is the cloud data and $X_2$ is the earthquake data. \n",
    "\n",
    "\n",
    "for the input data we have clouds = $0.8$ and earthquake = $5$ which when plugged into the formula above\n",
    "\n",
    "gives us: $44.9231$ = **Predicted Sharknado Hazard**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part G:** Consider the model you used in Part E, and consider the fact that you are trying to predict **sharknado hazard**. What is one critical drawback to the MLR model (or any MLR model) for predicting shardnado hazard? What are some modifications that could improve on this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Although MLR gives us a good model if we imagine that all the variables are independent, and all have their own slope to be found distinct and non-dependent on any other variable, in some datasets many features may be codependent; they may be complicated functions of eachother to some degree and to some degree be random or independent from one another, the MLR model gives us no way of letting the slopes be dependent on eachother; this assumption of independence is built in. **\n",
    "\n",
    "**Therefore, if we could analyze each slope vs each other slope and try to find codependencies there we might be able to have a far more accurate model with less overall error since it addresses the complex nature of the data at hand!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "[Back to top](#top)\n",
    "<a id='p3'></a>\n",
    "\n",
    "### [35 points] Problem 3: FlipMaster5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the file `flips.csv` you'll find the results of an experiment that was conducted with Stella O'Flaherty (the famous octopus data scientist) flipping coins. Her experiment was as follows. \n",
    "\n",
    "1. She reaches into her coin purse and grabs one of two coins, labeled $x$ and $y$. \n",
    "2. She flips her coin until it comes up heads 8 times, and records the coin ID and the number of flips it took to get 8 heads. \n",
    "3. She then replaces the coin in her coin purse and repeats the experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A:**\n",
    "\n",
    "By considering the total number of flips and the total number of \"heads\" in the data file for each coin, estimate the bias of each coin $p_x$ and $p_y$, and use an appropriate statistical test to determine whether the coins have the same bias, i.e. whether $p_x$ and $p_y$ are the same. Perform your test at a significance level that will mistakenly reject the null hypothesis _when that null hypothesis is actually true_ 5% of the time. Report a p-value for your test, and clearly state your conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**my method for estimating the paramters will be to take the proportion of the rolls that were heads to the number flips for each coin in total:\n",
    "so we just have to add up 8 times however many times we saw each coin out of the total number of coin flips that coin has been flipped!**\n",
    "\n",
    "note that $\\alpha$ = $0.05$\n",
    "\n",
    "so we get:\n",
    "\n",
    "\n",
    "$p_x$ = $\\frac{24}{9+13+12}$\n",
    "$p_y$ = $\\frac{16}{12+20}$\n",
    "\n",
    "$p_x$ = $\\frac{24}{34}$ = $0.7059$\n",
    "$p_y$ = $\\frac{16}{32}$ = $0.5$\n",
    "\n",
    "$n_x$ = $34$ flips for x\n",
    "\n",
    "and $n_y$ = $32$ flips for y\n",
    "\n",
    "each sample group actually has more than 30 total flips or more so we can use a z-distribution to represent the difference in sample proportions, \n",
    "$p_x$ - $p_y$\n",
    "\n",
    "# Calculating the p-value and what it tells us about our Test!\n",
    "\n",
    "Z = $\\frac{0.2059}{\\sqrt{\\frac{0.7059(1-0.7059)}{34} + \\frac{0.25}{32}}}$ = $1.74526$\n",
    "\n",
    "now to get the p-value we take:\n",
    "\n",
    "($1$ - **stats.norm.cdf**$(1.74526))x(2)$  = $0.08094$ which is MORE than our $\\alpha = 0.05$ \n",
    "\n",
    "the reason we are multiplying by 2 is because we need the probability for both tails of the test!\n",
    "\n",
    "so we cannot reject $H_o$ and we cannot claim that $P_x$ is really any different from $P_y$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Coin ID\\tFlips Required\n",
      "0                    x\\t9\n",
      "1                   x\\t13\n",
      "2                   x\\t12\n",
      "3                   y\\t12\n",
      "4                   y\\t20\n",
      "ts flipped:  9\n",
      "ts flipped:  13\n",
      "ts flipped:  12\n",
      "ts flipped:  12\n",
      "ts flipped:  20\n",
      "11.333333333333334 16.0\n",
      "120.0\n",
      "-0.025334608257501112 0.4371346082575011 0.2312346082575011\n",
      "z is:  1.7452577840363506  \n",
      " pval is:  0.08094000686110525\n"
     ]
    }
   ],
   "source": [
    "dfFlips = pd.read_csv(\"data/flips.csv\")\n",
    "print(dfFlips.head())\n",
    "x =[]\n",
    "y =[]\n",
    "isfirstp = True\n",
    "for lame_dataframe_row in dfFlips.values:\n",
    "    row = list(lame_dataframe_row[0])\n",
    "    coin = row[0]\n",
    "    if not isfirstp:\n",
    "        times_flipped = row[2] + row[3]\n",
    "        \n",
    "    else:\n",
    "        isfirstp = False\n",
    "        times_flipped = row[2]\n",
    "    if coin == 'x':\n",
    "        print(\"ts flipped: \", times_flipped)\n",
    "        x.append(int(times_flipped))\n",
    "    else:\n",
    "        print(\"ts flipped: \", times_flipped)\n",
    "        y.append(int(times_flipped))\n",
    "mean_flips_thx = np.mean(x)\n",
    "mean_flips_thy = np.mean(y)\n",
    "\n",
    "print(mean_flips_thx, mean_flips_thy)\n",
    "print(n_choose_r(10, 7))\n",
    "stats.norm.ppf(1-(0.05/2))\n",
    "\n",
    "term = 1.96*(np.sqrt(((0.7059*(1-0.7059))/34)+ (0.25/32)))\n",
    "\n",
    "print\n",
    "\n",
    "upper = .2059 + term\n",
    "lower = .2059 - term\n",
    "print(lower, upper, term)\n",
    "\n",
    "\n",
    "z = 0.2059/(np.sqrt((((0.7059)*(1-0.7059))/34) + (0.25/32)))\n",
    "pval = (1 - stats.norm.cdf(z))*2\n",
    "\n",
    "print(\"z is: \", z, \" \\n pval is: \", pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B:** \n",
    "\n",
    "You learn that, actually, the coin $x$ is from a manufacturer that produces coins whose biases follow some statistical regularity. In particular, the bias of the $x$ coin is in the set $$p_x \\in \\{0.1, 0.2, 0.3, \\dots, 0.9\\}.$$ Furthermore, these biases all occur with equal probability. In other words, $\\tfrac{1}{9}$ of coins have bias $p_x=0.1$, $\\tfrac{1}{9}$ of coins have bias $p_x=0.2$, and so on. \n",
    "\n",
    "For each possible value of $p_x$, compute the probability that Stella's $x$ coin has bias of $p_x$, given the data in her data file. \n",
    "\n",
    "Plot your results with $p_x$ on the horizontal axis and $Pr(p_x \\mid \\text{data})$ on the vertical axis. Make the points or lines that you plot blue. Plots without axis labels will receive zero credit.\n",
    "\n",
    "_Hint_: We have done problems like this before! Think back to how you solved the problem on the midterm where you determined the probability that someone had ESP, given that they guessed the cards correctly. There was a \"rule\", and maybe a \"law\" involved in your calculation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first lets use bayes rule to get a hold on this problem! we will also need law of total prob!\n",
    "\n",
    "let the event $(X_p == 0.1)$ represent the probability that the proportion of the x coin is 0.1\n",
    "\n",
    "let the event $(\\frac{24}{34}H)$ represent the probability that we flipped the x coin 24/34 times as a head OVERALL\n",
    "\n",
    "ok so what we are trying to compute for each $\\hat{X_p} $ = ${.1,.2,.3,.4,.5,...,.9}$ \n",
    "\n",
    "$P((X_p == \\hat{X_p}) \\mid (\\frac{24}{34}H))$ \n",
    "\n",
    "this is the term I will apply bayes rule to!\n",
    "\n",
    "applying bayes rule we get:\n",
    "\n",
    "$P((X_p == \\hat{X_p}) \\mid (\\frac{24}{34}H))$  = $\\frac{P((\\frac{24}{34}H)\\space \\mid \\space (X_p == \\hat{X_p}))P(X_p == \\hat{X_p})}{P(\\frac{24}{34}H)}$\n",
    "\n",
    "\n",
    "alright, we almost have the general term to calculate each and every probability for each $\\hat{X_p}$ in the set,\n",
    "\n",
    "why? because the term $P((\\frac{24}{34}H)\\space \\mid \\space (X_p == \\hat{X_p}))$ is just a negative binomial distribution with a specific r, k, and p values\n",
    "\n",
    "$P((\\frac{24}{34}H)\\space \\mid \\space (X_p == \\hat{X_p}))$ = ${r-1}\\choose{k-1}$$p^r$$(1-p)^{k-r}$ where r = 24, k = 34 and p = $\\hat{X_p}$, and we also know that $P(X_p == \\hat{X_p})$ = $\\frac{1}{9}$ for all values of $\\hat{X_p}$\n",
    "\n",
    "\n",
    "so the expression becomes:\n",
    "\n",
    "$\\frac{{{k-1}\\choose{r-1}}p^r(1-p)^{k-r}P(X_p == \\hat{X_p})}{P(\\frac{24}{34}H)}$\n",
    "\n",
    "$\\frac{{{33}\\choose{23}}(\\hat{X_p})^{24}(1-\\hat{X_p})^{10}\\frac{1}{9}}{P(\\frac{24}{34}H)}$\n",
    "\n",
    "\n",
    "now overall we have:\n",
    "\n",
    "$P((X_p == \\hat{X_p}) \\mid (\\frac{24}{34}H))$ = $\\frac{{{33}\\choose{23}}(\\hat{X_p})^{24}(1-\\hat{X_p})^{10}\\frac{1}{9}}{P(\\frac{24}{34}H)}$\n",
    "\n",
    "\n",
    "the next thing we have to do is to apply the law of total probability to the bottom of the fraction. \n",
    "\n",
    "\n",
    "the bottom of the fraction, $P(\\frac{24}{34}H)$ is really the sum of disjoint events in the probability space:\n",
    "\n",
    "that is $P(\\frac{24}{34}H)$ = $\\Sigma_{\\hat{X_p} \\in {0.1,...,0.9}}{P(\\frac{24}{34}H \\mid X_p=\\hat{X_p})\\frac{1}{9}}$\n",
    "\n",
    "\n",
    "so the final, general form equation to calculate the probability of each value in the set is:\n",
    "\n",
    "\n",
    "$P((X_p == \\hat{X_p}) \\mid (\\frac{24}{34}H))$ = $\\frac{{{33}\\choose{23}}(\\hat{X_p})^{24}(1-\\hat{X_p})^{10}\\frac{1}{9}}{\\Sigma_{\\hat{X_p} \\in {0.1,...,0.9}}{P(\\frac{24}{34}H \\mid X_p=\\hat{X_p})\\frac{1}{9}}}$\n",
    "\n",
    "the 1/9ths cancel and we get:\n",
    "\n",
    "$P((X_p == \\hat{X_p}) \\mid (\\frac{24}{34}H))$ = $\\frac{{{33}\\choose{23}}(\\hat{X_p})^{24}(1-\\hat{X_p})^{10}}{\\Sigma_{\\hat{X_p} \\in {0.1,...,0.9}}{P(\\frac{24}{34}H \\mid X_p=\\hat{X_p})}}$\n",
    "\n",
    "\n",
    "but $P(\\frac{24}{34}H \\mid X_p=\\hat{X_p})$ is just = ${{33}\\choose{23}}(\\hat{X_p})^{24}(1-\\hat{X_p})^{10}\\frac{1}{9}$\n",
    "\n",
    "so the formula then becomes:\n",
    "\n",
    "$P((X_p == \\hat{X_p}) \\mid (\\frac{24}{34}H))$ = $\\frac{{{33}\\choose{23}}(\\hat{X_p})^{24}(1-\\hat{X_p})^{10}}{\\Sigma_{\\hat{X_p} \\in {0.1,...,0.9}}{{{33}\\choose{23}}(\\hat{X_p})^{24}(1-\\hat{X_p})^{10}}}$\n",
    "\n",
    "\n",
    "now we can let python calculate the value for each value of $\\hat{X_p}$\n",
    "= all values from python in one array = [1.5997448017189437e-16, 8.265047980417788e-10, 3.6602953207363036e-06, 0.0007808685091131031, 0.026705810356336474, 0.22795836622095855, 0.5190283984622512, 0.22186319234853222, 0.003659702980982689]\n",
    "\n",
    "as python has so graciously calculated the values are:\n",
    "\n",
    "\n",
    "$P((X_p == 0.1) \\mid (\\frac{24}{34}H))$ = $1.5997448017189437e-16$\n",
    "\n",
    "\n",
    "$P((X_p == 0.2) \\mid (\\frac{24}{34}H))$ = $8.265047980417788e-10$\n",
    "\n",
    "\n",
    "$P((X_p == 0.3) \\mid (\\frac{24}{34}H))$ = $3.6602953207363036e-06$\n",
    "\n",
    "\n",
    "$P((X_p == 0.4) \\mid (\\frac{24}{34}H))$ = $0.0007808685091131031$\n",
    "\n",
    "\n",
    "$P((X_p == 0.5) \\mid (\\frac{24}{34}H))$ = $0.026705810356336474$\n",
    "\n",
    "\n",
    "$P((X_p == 0.6) \\mid (\\frac{24}{34}H))$ = $0.227958366220958554$\n",
    "\n",
    "\n",
    "$P((X_p == 0.7) \\mid (\\frac{24}{34}H))$ = $0.5190283984622512$\n",
    "\n",
    "\n",
    "$P((X_p == 0.8) \\mid (\\frac{24}{34}H))$ = $0.22186319234853222$\n",
    "\n",
    "\n",
    "$P((X_p == 0.9) \\mid (\\frac{24}{34}H))$ = $0.003659702980982689$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the respective values of xp:  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] \n",
      "\n",
      "results indexed the same [1.5997448017189437e-16, 8.265047980417788e-10, 3.6602953207363036e-06, 0.0007808685091131031, 0.026705810356336474, 0.22795836622095855, 0.5190283984622512, 0.22186319234853222, 0.003659702980982689]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ps=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "p = [1/9 for i in range(1, 10)]\n",
    "def get_pxp_given_kofrheads(xp, ps, k=34, r=24):\n",
    "    top_part = n_choose_r(33, 23)*(xp**24)*((1-xp)**10)\n",
    "    bot_part = sum([n_choose_r(33, 23)*(i**24)*((1-i)**10) for i in ps])\n",
    "    if top_part < bot_part:\n",
    "        return top_part / bot_part\n",
    "    else:\n",
    "        print(\"error: top of fraction somehow bigger than bottem of fraction....\")\n",
    "        print(\"top: \", top_part, \" bottom: \", bot_part)\n",
    "        return 0.0\n",
    "results_x = []\n",
    "for xp in ps:\n",
    "    results_x.append(get_pxp_given_kofrheads(xp, ps))\n",
    "    \n",
    "print(\"the respective values of xp: \", ps, \"\\n\\nresults indexed the same\",  results_x)\n",
    "\n",
    "plt.scatter(ps, results_x)\n",
    "plt.xlabel(\"values of Px\")\n",
    "plt.ylabel(\"probability of P(Px = Px | 24/34 heads\")\n",
    "plt.grid(True, color='g', linestyle='-', linewidth=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C:**\n",
    "\n",
    "You learn that, actually, the coin $y$ is from a different manufacturer that produces coins whose biases follow some statistical regularity. In particular, the bias of the $y$ coin is in the set $$p_y \\in \\{0.1, 0.2, 0.3, \\dots, 0.9\\}.$$ Furthermore, these biases all occur with different probability. In particular, the probability that a coin has bias $p_y$ is proportional to $p_y$, which could be written as \n",
    "$$Pr(p_y) \\propto p_y \\quad \\text{for} \\quad p_y \\in \\{0.1, 0.2, 0.3, \\dots, 0.9\\}$$\n",
    "\n",
    "First, write clearly the PMF for $p_y$, based on the information above. \n",
    "\n",
    "Then, for each possible value of $p_y$, compute the probability that Stella's $y$ coin has bias of $p_y$, given the data in her data file. \n",
    "\n",
    "Plot your results with $p_y$ on the horizontal axis and $Pr(p_y \\mid \\text{data})$ on the vertical axis. Make the points or lines that you plot red. Plots without axis labels will receive zero credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us agani apply bayes rule, remembering that for the y coin we had a total of 16 heads and flips overall. (implying also 16 tails)\n",
    "\n",
    "let $P(y_p = \\hat{y_p})$ signify the probability that the propotion of y is one of the values in the set. \n",
    "\n",
    "\n",
    "and let $P(\\frac{16H}{32})$ signify the probability that we got 16H out of 32 tries.\n",
    "\n",
    "let $S$ be the set with all values of $\\hat{y_p} \\in S$\n",
    "\n",
    "\n",
    "$P(y_p = \\hat{y_p} \\mid \\frac{16H}{32} ) = \\frac{P(\\frac{16H}{32} \\mid y_p = \\hat{y_p})P(y_p = \\hat{y_p})}{P(\\frac{16H}{32})}$\n",
    "\n",
    "first this time lets apply the law of total probability to the bottom of the fraction:\n",
    "\n",
    "\n",
    "we know that $P(\\frac{16H}{32})$ = $\\Sigma_{\\hat{y_p} \\in S}{P(\\frac{16H}{32} \\space |  \\space y_p = \\hat{y_p})P(y_p = \\hat{y_p})}$\n",
    "\n",
    "so now we have:\n",
    "\n",
    "$ = \\frac{P(\\frac{16H}{32} \\mid y_p = \\hat{y_p})P(y_p = \\hat{y_p})}{\\Sigma_{\\hat{y_p} \\in S}{P(\\frac{16H}{32} \\space |  \\space y_p = \\hat{y_p})P(y_p = \\hat{y_p})}}$\n",
    "\n",
    "now, we can use the negative binomial distribution again, to get values for $P(\\frac{16H}{32} \\mid y_p = \\hat{y_p})$\n",
    "\n",
    "for each value of $\\hat{y_p}$.\n",
    "\n",
    "\n",
    "$P(y_p = \\hat{y_p} \\mid \\frac{16H}{32} ) = \\frac{{{31}\\choose{15}}{\\hat{y_p}^{16}}{{(1-\\hat{y_p})}^{16}}P(y_p = \\hat{y_p})}{\\Sigma_{\\hat{y_p} \\in S}{P(\\frac{16H}{32} \\space |  \\space y_p = \\hat{y_p})P(y_p = \\hat{y_p})}}$\n",
    "\n",
    "\n",
    "\n",
    "now we can replace $P(y_p = \\hat{y_p})$ with $\\hat{y_p}$ since this is proportional the probability of it having that value\n",
    "\n",
    "now we can also resplace $P(\\frac{16H}{32} \\space |  \\space y_p = \\hat{y_p})$ in the denominator with:\n",
    "\n",
    "\n",
    "${{31}\\choose{15}}{\\hat{y_p}^{16}}{{(1-\\hat{y_p})}^{16}}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$P(y_p = \\hat{y_p} \\mid \\frac{16H}{32} ) = \\frac{{{31}\\choose{15}}{\\hat{y_p}^{16}}{{(1-\\hat{y_p})}^{16}}\\hat{y_p}}{\\Sigma_{\\hat{y_p} \\in S}{{{31}\\choose{15}}{\\hat{y_p}^{16}}{{(1-\\hat{y_p})}^{16}}\\hat{y_p}}}$\n",
    "\n",
    "\n",
    "now that I have the general form, I convert it to python with the code below and let python fill up my results array\n",
    "with all the probabilities that the Y coin has each bias given the data!\n",
    "\n",
    "my results array was: [7.351176475615182e-09, 0.00014636135310578625, 0.017025766312305384, 0.19227222693869797, 0.46183499800161, 0.2884083404080469, 0.039726788062046006, 0.0005854454124231425, 6.616058828053635e-08]\n",
    "\n",
    "which corresponds to my final results of:\n",
    "\n",
    "\n",
    "$P(y_p = 0.1 \\mid \\frac{16H}{32}) = 7.351176475615182e-09$\n",
    "\n",
    "\n",
    "$P(y_p = 0.2 \\mid \\frac{16H}{32}) = 0.00014636135310578625$\n",
    "\n",
    "\n",
    "$P(y_p = 0.3 \\mid \\frac{16H}{32}) = 0.017025766312305384$\n",
    "\n",
    "\n",
    "$P(y_p = 0.4 \\mid \\frac{16H}{32}) = 0.19227222693869797$\n",
    "\n",
    "\n",
    "$P(y_p = 0.5 \\mid \\frac{16H}{32}) = 0.46183499800161$\n",
    "\n",
    "\n",
    "\n",
    "$P(y_p = 0.6 \\mid \\frac{16H}{32}) = 0.2884083404080469$\n",
    "\n",
    "\n",
    "$P(y_p = 0.7 \\mid \\frac{16H}{32}) = 0.039726788062046006$\n",
    "\n",
    "\n",
    "$P(y_p = 0.8 \\mid \\frac{16H}{32}) = 0.0005854454124231425$\n",
    "\n",
    "\n",
    "\n",
    "$P(y_p = 0.9 \\mid \\frac{16H}{32}) = 6.616058828053635e-08$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph is plotted below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.351176475615182e-09, 0.00014636135310578625, 0.017025766312305384, 0.19227222693869797, 0.46183499800161, 0.2884083404080469, 0.039726788062046006, 0.0005854454124231425, 6.616058828053635e-08]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getProb(numHeads, numFlips, Yp, YpArray):\n",
    "    top_frac = n_choose_r(numFlips-1, numHeads-1)*(Yp**numHeads)*((1-Yp)**(numFlips-numHeads))*Yp\n",
    "    bottom_frac = sum([(n_choose_r(numFlips-1, numHeads-1)*(Yi**numHeads)*((1-Yi)**(numFlips-numHeads))*Yi) for Yi in YpArray])\n",
    "    return top_frac / bottom_frac\n",
    "\n",
    "YpArray = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "results = [getProb(16, 32, Yval, YpArray) for Yval in YpArray]\n",
    "print(results)\n",
    "plt.scatter(YpArray, results)\n",
    "plt.xlabel(\"values of coin y's bias\")\n",
    "plt.ylabel(\"P(bias | data)\")\n",
    "plt.grid(True, color='g', linestyle='-', linewidth=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D:**\n",
    "\n",
    "The information that you have about the manufacturer of coin $x$ and coin $y$ is called _prior information_ since it can influence the estimates of a coin's bias at which you arrive, given the data from the coin's flipping. We often call the distribution $Pr(p_x)$ or $Pr(p_y)$ a _prior distribution_, and call $Pr(p_x \\mid \\text{data})$ or $Pr(p_y \\mid \\text{data})$ a _posterior distribution_, since it represents the estimate that you arrive at after you have taken the data into account. \n",
    "\n",
    "You have already computed posterior distributions for each coin's bias. However, you'll now investigate the importance of the prior by _switching the priors for the two coins_.\n",
    "\n",
    "In other words, using the prior probabilities $Pr(p_x)$, what is your posterior distribution of $Pr(p_y \\mid \\text{data from y})$? Similarly, using the prior probabilities $Pr(p_y)$, what is your posterior distribution of $Pr(p_x \\mid \\text{data from x})$? \n",
    "\n",
    "Create two plots. \n",
    "\n",
    "1. In the first plot, show your results from Part B (the posterior distribution for $p_x$ with the correct prior) plotted with a blue solid line as well as your results from Part D for the posterior distribution for $p_x$ with the incorrect prior with a blue dashed line.  \n",
    "\n",
    "2. In the second plot, show your results from Part C (the posterior distribution for $p_y$ with the correct prior) with a red solid line as well as your results from Part D for the posterior distribution for $p_y$ with the incorrect prior with a red dashed line.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3032972190833458e-17, 2.3799873590376906e-10, 1.5810174268202238e-06, 0.0004497147954042127, 0.019225384780441952, 0.19692736132138264, 0.5231044763284638, 0.2555491967990066, 0.0047422847198752525]\n",
      "[7.351176475615182e-09, 0.00014636135310578625, 0.017025766312305384, 0.19227222693869797, 0.46183499800161, 0.2884083404080469, 0.039726788062046006, 0.0005854454124231425, 6.616058828053635e-08]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f61e14ceb8>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "ax[0].plot(ps, results_x)\n",
    "ax[0].grid(True, color='g', linestyle='-', linewidth=0.25)\n",
    "\n",
    "ax[1].plot(YpArray, results, color='r')\n",
    "ax[1].grid(True, color='g', linestyle='-', linewidth=0.25)\n",
    "\n",
    "y_new_posterior = [getProb(24, 34, xi, YpArray) for xi in YpArray]\n",
    "print(y_new_posterior)\n",
    "\n",
    "ax[1].plot(ps, y_new_posterior, linestyle='dashed', color='r')\n",
    "\n",
    "\n",
    "x_new_posterior = [getProb(16, 32, xi, ps) for xi in ps]\n",
    "print(x_new_posterior)\n",
    "\n",
    "ax[0].plot(YpArray, x_new_posterior, linestyle='dashed', color='b')\n",
    "\n",
    "#def getProb(numHeads, numFlips, Yp, YpArray):\n",
    " #   top_frac = n_choose_r(numFlips-1, numHeads-1)*(Yp**numHeads)*((1-Yp)**(numFlips-numHeads))*Yp\n",
    " #   bottom_frac = sum([(n_choose_r(numFlips-1, numHeads-1)*(Yi**numHeads)*((1-Yi)**(numFlips-numHeads))*Yi) for Yi in YpArray])\n",
    " #  return top_frac / bottom_frac\n",
    "\n",
    "#YpArray = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "#results = [getProb(16, 32, Yval, YpArray) for Yval in YpArray]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the prior $Pr(P_y)$ = $\\{0.1, 0.2, ... 0.9\\}$\n",
    "\n",
    "and the prior $Pr(P_x)$ = $\\{\\frac{1}{9} \\space \\mid \\space e \\in \\hat{S}\\}$ where $\\hat{S}$ is the set of biases for $P_x$.\n",
    "\n",
    "\n",
    "so, then, what does it mean to switch these priors? \n",
    "it means to calcualte the P(Pr(Py=Px) for each value of Px)\n",
    "and to calcluate the P(Pr(Px=Py for each value of Py) and graph those, and you just end up graphing the other distribution.\n",
    "\n",
    "this is an interesting result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E:**\n",
    "\n",
    "What is the name of the distribution that Stella's experiment is drawn from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Negative Binomial Distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "[Back to Problem 2](#p2)\n",
    "\n",
    "<a id='footnote'></a> Yeah yeah - fresh water versus salt water - I know, I know. But sharknadoes also are not real, so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
