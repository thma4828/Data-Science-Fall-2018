{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Review\n",
    "- suppose you use stats.models and get some results:\n",
    "- n = 50 \n",
    "- const = 1.6573 (correct you know how to read the table)\n",
    "- X slope: 2.0667\n",
    "- give brief interp of the slope param:\n",
    "- well, we can see that the x param has a positive slope of 2.0667, with respec tto the y varaible,\n",
    "- so for every one increase in x we go up 2.0067 in the response var. \n",
    "- also we notice that the fstatistic is large, and the p-value implied by the f-stat is small so therefore the one feature we have should be statistically significant compared to the response value y. \n",
    "\n",
    "\n",
    "\n",
    "# how do we compute 95% CI for slope parameter of a SLR REGRESSION??\n",
    "- upper bound \n",
    " - $\\hat{B} + t_{\\frac{\\alpha}{2}, n-2}SE(\\hat{B})$\n",
    "- lower bound\n",
    " - $\\hat{B} - t_{\\frac{\\alpha}{2}, n-2}SE(\\hat{B})$\n",
    " \n",
    " \n",
    " \n",
    " $SE(\\hat{B})$ = $\\sqrt{\\frac{SSE_{n-2}}{\\Sigma{(x_i - \\hat{x})^2}}}$ = 0.106 (this came from the output in the problem!!)\n",
    " \n",
    " B += $t_{{0.025, 48}}$$0.106$ \n",
    " \n",
    " \n",
    " $t_{{0.025, 48}}$ comes from a table = $2.011$, so if we wanted p-value $1 - stats.t.cdf(2.011, 48)$\n",
    " \n",
    " upper: $\\hat{B}$ + $2.011$$0.106$\n",
    " \n",
    " \n",
    "  lower: $\\hat{B}$ - $2.011$$0.106$ \n",
    "  \n",
    "  \n",
    "  \n",
    "  what fraction of the total variation in the response is not explained by the SLR model? \n",
    "  \n",
    "  === 1 - $R^2$ = 0.1 if $R^2$ - 0.9!\n",
    "  \n",
    "  \n",
    "  $R^2$ vs adjusted $R^2$ the key here is that the number of params is small so the adj wont do much\n",
    "  \n",
    "  \n",
    "  this is a simple question we just want to know how well we fit the data same as $R^2$. \n",
    "  \n",
    "  \n",
    "  \n",
    "  # narrowness of CI:\n",
    "  - known variance --> Z-dist, $Z_{\\frac{\\alpha}{2}}$\n",
    "  - think of alpha as our false positive rate aka the CI doesnt capture the true value alpha percent of the  time. \n",
    "  - so if given the percent that a CI fails you are being given its alpha level\n",
    "  - a smaller CI is more tollerant of false positives because the alpha level is greater!\n",
    "  \n",
    "  \n",
    "  \n",
    "  # Plinko Problems aka Bernouli Trials ~B()...\n",
    "  - remember binomial distr\n",
    "  - 100\\$ won occurs with the probability $(0.5)^4$ = 0.0625\n",
    "  - 50\\$ won occurs with the probability 0.0625\n",
    "  \n",
    "  \n",
    "  \n",
    "  # t-test vs z-test\n",
    "  \n",
    "  # Z Test\n",
    "  - **we use a z-test if: **\n",
    "      - we have normal data, known $\\sigma$ and $N \\ge 30$\n",
    "      - we have normal data, known $\\sigma$ and $N \\le 30$\n",
    "      - we have non-normal data, known $\\sigma$ and $N \\ge 30$\n",
    "      - we have non-normal data, unknown $\\sigma$ and $N \\ge 30$\n",
    "      \n",
    "  - **the Z-score for a mean is: **\n",
    "      - $\\frac{\\hat{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}$\n",
    "          \n",
    "  - **the Z-Confidence Interval for the mean is:**\n",
    "      - lower bound: $\\hat{X} + Z_{\\frac{\\alpha}{2}}(\\frac{S}{\\sqrt{n}})$\n",
    "      - upper bound: $\\hat{X} - Z_{\\frac{\\alpha}{2}}(\\frac{S}{\\sqrt{n}})$\n",
    "  - **the z-score for difference in ps is:**\n",
    "      - $\\frac{\\hat{P} - P_i}{\\sqrt{\\frac{P_i(1-P_i)}{n}}}$\n",
    "      \n",
    "  - **the pvalue for an observed z-score is either:**\n",
    "      - 1 - $\\Phi(Z)$ if $H_1$ is that the observed param, $p > p_0$\n",
    "      - $\\Phi(Z)$ if $H_1$ is that the observed param $p < p_0$\n",
    "      - 2$\\Phi(-1(|Z|))$ if $H_1$ is that $p \\ne p_0$\n",
    "  \n",
    "  \n",
    "# t test\n",
    " - we use a t-test if:\n",
    "    ** - we have reason to believe data is normal but we have unkown $\\sigma$ and if n < 30.**\n",
    " - so we have a small sample size, and we computa a t-score similarly to the way we do other tests but with a little bit of a difference.\n",
    "     - when we want a confidence interval, or hypothesis test about the MEAN of a population where we have a small sample that is not known to be normally distributed. \n",
    " \n",
    "  - **t rejection region depends on the tails of test as usual:**\n",
    "        - if: $H_1$ is that v > $v_0$:\n",
    "            - if the observed t-value is greater than the value of $T_{\\alpha, n-1}$\n",
    "        - if: $H_1$ is that v < $v_0$ \n",
    "            - if the observed t-vakue is less than the value of -$T_{\\alpha, n-1}$\n",
    "        - if: $H_1$ is that $v \\ne v_0$\n",
    "            - if the observed t-value is greater than the value of $T_{\\frac{alpha}{2}, n-1}$\n",
    "            \n",
    "            \n",
    "  - **t-CI interval for the mean:**\n",
    "      - $H_1$ is that $v \\ne v_o$ by default\n",
    "      - lower: $\\hat{X}$ $-$ $t_{\\frac{\\alpha}{2}, n-1}\\frac{S}{\\sqrt{n}}$\n",
    "      - upper: $\\hat{X}$ $+$ $t_{\\frac{\\alpha}{2}, n-1}\\frac{S}{\\sqrt{n}}$\n",
    "  \n",
    "  - **the t-score for difference in ps is:**\n",
    "      - t = $\\frac{\\hat{P} - P_i}{\\sqrt{\\frac{P_i(1-P_i)}{n}}}$\n",
    "      -- now just make sure you are comparing this to a t statistic.\n",
    "      \n",
    "  - **p-values for a t-test**\n",
    "      - 1 - t.cdf($t$, $n-1$) if $H_1$ is that the observed param, $p > p_0$\n",
    "      - t.cdf($t$, $n-1$) if $H_1$ is that the observed param $p < p_0$\n",
    "      - 2(t.cdf(-1 x |$t$|, $n-1$))) if $H_1$ is that $p \\ne p_0$\n",
    "      \n",
    "      \n",
    "      notice here we have to use the cdf for a t-dist!\n",
    "      \n",
    "   - example: in python a single tailed test would be computed by:\n",
    "    - alpha = 0.05 #false positive rate (rate we reject H0 when we should not type I error)\n",
    "    - n = 22 #sample size\n",
    "    - tval = stats.t.ppf(1 - alpha, n-1) #gets T_alpha, ddof\n",
    "    - pval = 1 - stats.t.cdf(tval, n-1) #get the p-value from the t-distribution with the cdf.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so computing a t-test just means we have to go to the t-distribution to get our paramter of $t_{\\alpha/2, n-1}$\n",
    "and we need to know when to apply it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variances and the Chi2 test (Chi-Squared). \n",
    "- Q: what is the sampling distribution of the variances of a population that is normally distributed?\n",
    "- A: the variances of a normal population follow a sampling distribution of $\\chi^2(v)$ where v = n-1 is DOF, and n is the sample size. \n",
    "\n",
    "- how would we compute a 100(1-alpha)% CI for the variance???\n",
    "    - $\\frac{(n-1)(S^2)}{\\chi^2_{\\frac{\\alpha}{2}, n-1}} \\space \\lt \\space \\sigma^2 \\space \\lt \\space \\frac{(n-1)(S^2}{\\chi^2_{1 - \\frac{\\alpha}{2}, n-1}}$\n",
    "    \n",
    "    - where $S^2$ is the sample variance: $\\frac{1}{n-1}\\Sigma{(x_i - \\hat{x})}$ where $\\hat{X}$ is sample mean. and n is sample size. \n",
    "    - where $\\chi^2_{\\frac{\\alpha}{2}, n-1}$ is the right value from the $\\chi^2$ distribution and \n",
    "    - where $\\chi^2_{1 - \\frac{\\alpha}{2}, n-1}$ is the left value from the $\\chi^2$ distribution.\n",
    "    \n",
    "    \n",
    "- if the left and right side get confused about which is bigger and which is smaller you can just flip the CI around to make it make sense since this is due to some inverse reading of the distributions in a sense. So notice the symmetry in the distribution and you can swap right and left to make the arrows make sense since this is part of what happend when we simplified its ok you can just fix the mistake. \n",
    "\n",
    "\n",
    "\n",
    "- we use the Chi distribution when we want to test hypothesis about the variance of a population that is known to be normally distributed even when we have a small sample size!!!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.02276779864163 2.7003894999803584\n",
      "37.800000000000004\n"
     ]
    }
   ],
   "source": [
    "chiValleftSide = stats.chi2.ppf(1 - 0.025, 9)\n",
    "chiValrightSide = stats.chi2.ppf(0.025, 9)\n",
    "\n",
    "print(chiValleftSide, chiValrightSide)\n",
    "print(9*4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap\n",
    "- if you have a small sample size and you want to calculate a 95% CI for the mean, standard deviation, median, mode etc.\n",
    "- we use bootstrap when:\n",
    "    - n < 30\n",
    "    - when we have non-normal data with a known or unkown $\\sigma$ value. \n",
    "- here are the steps to bootstrap:\n",
    "    - resample your original data many times into N different resamples: {$R_1$ ... $R_n$}\n",
    "    - compute the mean of each resample, $R_j$. \n",
    "    - compute the 2.5 and 97.5 percentiles for all these calculated means.\n",
    "    - CI: [a, b] where a = 2.5 percentile, b = 97.5 percentile\n",
    "    - the median is the 50th percentile, but how do you get the 2.5? \n",
    "    -50 -- 75 -- 87.5 -- 93.75 --\n",
    "    \n",
    "# Linear Regression:\n",
    "- we have some data $(x_i, y_i)$, we want to predict the y (response variable) based on the x's (feature variable(s)).\n",
    "- example SLR: = single linear regression (1 feature only) there is always only one response!\n",
    "- given our data we want to approximate the model parameters:\n",
    "    - $\\hat{\\alpha}$ \n",
    "    - $\\hat{\\beta}$ \n",
    "- for the model: \n",
    "    - $y$ = $\\hat{\\alpha}$  + $\\hat{\\beta}x$ \n",
    "- we can approximate $\\hat{\\beta}$ = $\\frac{\\Sigma_{i=1}^n{(x_i - \\hat{x})(y_i - \\hat{y})}}{\\Sigma_{i=1}^n{(x_i - \\hat{x})}}$\n",
    "- we can approximate $\\hat{\\alpha}$ = $\\hat{y} - \\hat{\\beta}{\\hat{x}}$\n",
    "\n",
    "\n",
    "where $\\hat{x}$ is the mean value of $x$'s in our dataset.\n",
    "and where $\\hat{y}$ is the mean value of $y$'s in our dataset. \n",
    "\n",
    "# SLR: Single Linear Regression\n",
    "- model is: y = $\\alpha + \\beta(x)$\n",
    "- **SSE** = $\\Sigma_{i=1}^{n}{(y_i - (\\alpha + \\hat{B}x)})^2$\n",
    "- **SST** = $\\Sigma_{i=1}^{n}{(y_i - \\hat{y})^2}$\n",
    "- $R^2$ = $1 - \\frac{SSE}{SST}$ \n",
    "- $R^2$ can by thought of as 1 minus the ratio of models error to the error given by an unfitted model with a slope of 0. 1 - modelError/originalError\n",
    "- we can approximate $\\hat{\\beta}$ = $\\frac{\\Sigma_{i=1}^n{(x_i - \\hat{x})(y_i - \\hat{y})}}{\\Sigma_{i=1}^n{(x_i - \\hat{x})}}$\n",
    "- we can approximate $\\hat{\\alpha}$ = $\\hat{y} - \\hat{\\beta}{\\hat{x}}$\n",
    " \n",
    "# MLR: regression with multiple features\n",
    "- $Y = B_0 + B_1(X_1) + B_2(X_2) + ... + B_k(X_k)$\n",
    "- we are still reaching our parameters {$B_0 .. B_k$} by minimizing the SSE.\n",
    "- the only way we saw in class of finding the optimal params are by gradient descent, (see first part of this notebook!)\n",
    "\n",
    "- when is a linear relationship not sufficient??\n",
    "    - when features both correlate to increase or decrease response in an exponential or non-linear fashion.\n",
    "    \n",
    "# MLR: correlation of features \n",
    "- covariance: \n",
    "    - let X and Y be random variables. \n",
    "    - the Covariance; Cov(X, Y) = $E[(X - E[X])(Y - E[Y])]$\n",
    "    - the expected value of a discrete random variable:\n",
    "         - $\\Sigma_{X_i \\in S}{P(X == X_i)(X_i)}$ where $S$ is the set of all possible values of the variable $X$, $S$ = **{$X_0$....$X_k$}**\n",
    "    - the expected value of a continuous random variable:\n",
    "         - $\\int_{a}^{b}{{x}{(f(x)}){dx}}$\n",
    "         - where [a, b] is the whole range of the PMF.\n",
    " - correlation coef p:\n",
    "     - a measure of correlation of random variables X, Y\n",
    "     - $p(X,  Y)$ = p = $\\frac{Cov(X, Y)}{\\sqrt{Var(x)Var(y)}}$\n",
    "\n",
    "# F-Testing with MLR: is one slope non-zero?\n",
    "- $Y = B_0 + B_1(X_1) + B_2(X_2) + B_3(X_3) + ... + B_p(X_p)$\n",
    "- $H_0: B_0 == B_1 ... == B_p == 0$\n",
    "- $H_1:$ At least one $B$, $B_j$ such that $j \\ne 0$ is non-zero\n",
    "\n",
    " - p = x - (n -p - 1)\n",
    " - p = x - n + p + 1\n",
    " - -x = -n + 1\n",
    " - x = n - 1\n",
    "- $dfSST$ = n-1 degrees of freedome for SST\n",
    "- $dfSSE$ = n-p-1 = degrees of freedome for SSE\n",
    "- $F$ = $\\frac{\\frac{SST-SSE}{dfSST - dfSSE}}{\\frac{SSE}{dfSSE}}$\n",
    "- ---------------------------------------------------------\n",
    "- rewritten as: $F$ = $\\frac{\\frac{SST-SSE}{p}}{\\frac{SSE}{n - p - 1}}$\n",
    "- interpreting the F-value:\n",
    "    - if $H_0$ was true: F would be about 1\n",
    "    - if $H_1$ was true: F would be than 1\n",
    "- interpreting the P-value:\n",
    "    - same as the p-value for anything else = the probability we observed this result assuming that $H_0$ was true.\n",
    "    - note that the F-stat has 2 different DOF parameters. \n",
    "    -** also note that F is ALWAYS a 1-tailed test. ** so the pvalue should be interpreted as such. \n",
    "    \n",
    "    \n",
    "# F-test: part 2: are missing features important?\n",
    "- suppose we have chosen only a subset of features. \n",
    "- suppose further from a set of $(X_1, X_2, X_3, X_4)$ we got rid of features $X_1$ and $X_3$ and their corresponding slopes $B_1, B_3$, so now we have the model: $Y = B_0 + B_2(X_2) + B_4(X_4)$\n",
    "- $H_0: $ $B_1 = B_3 = 0$\n",
    "- $H_1: $ is that the subset of features was important, that both slopes are not equal to zero!\n",
    "- in this case the full model has p = 4 features\n",
    "- in this case the reduced model has k = 2 features,\n",
    "- n = sample size\n",
    "- F = $\\frac{\\frac{SSE_red - SSE_full}{p-k}}{\\frac{SSE_full}{n-p-1}}$\n",
    "- Rejection Region: $F \\ge F_{(\\alpha)(p-k)(n-p-1)}$\n",
    "- AGAIN you just calculate the F value you get from your data and compare it to the F-value from the table that corresponds to the rejection region!\n",
    "- $R^2$ for MLR is still just: $1 - \\frac{SSE}{SST}$ but now we have to accound for the number of features:\n",
    "- $R^2$ for MLR ends up being: $1 - \\frac{\\frac{SSE}{dfSSE}}{\\frac{SST}{dfSST}}$\n",
    "- dfSSE = n - p - 1 \n",
    "- dfSST = n - 1\n",
    "\n",
    "# ANOVA\n",
    "- comparing multiple means of > 2 populations.\n",
    "- we can determine if any of the means are different using a method called ANOVA. \n",
    "- ANOVA does this by Analyzing the Variances of the populations as well as the overall population variance.\n",
    "- suppose we have I groups of data. \n",
    "- let $Y_{(i, j)}$ represents the $j^{th}$ data point for the response variable in the $i^{th}$ group of data.\n",
    "- let $U_{i}$ represent the mean of the $i^{th}$ group of data.\n",
    "- let $E_{(i, j)}$ **~** $N(0, \\sigma^2)$\n",
    "- $Y_{(i, j)}$ = $U_{i}$ + $E_{(i, j)}$\n",
    "- the above formula represents the relationship between responses and group means...\n",
    "- the Grand mean, $\\bar{\\bar{Y}}$ = mean of all Y (response)\n",
    "- let group $i$'s mean for Y (response) be represented by $\\bar{Y_i}$\n",
    "- the total variation still comes from SST which has this formula now:\n",
    "- **SST** = $\\Sigma_{i=1}^{I}\\Sigma_{j=1}^{n}(y_{i,j})(\\bar{\\bar{Y}})^2$\n",
    "- now we need the **between group sum of squares. **\n",
    "- **SSB** = $\\Sigma_{i=1}^{I}\\Sigma_{j=1}^{n}(\\bar{Y_i} - \\bar{\\bar{Y}})^2$\n",
    "- now we need the **within group sum of squares. **\n",
    "- **SSW** = $\\Sigma_{i=1}^{I}\\Sigma_{j=1}^{n}(y_{i,j} - \\bar{Y_i})^2$\n",
    "- **SST** = **SSB + SSW**\n",
    "- **the degrees of freedom for SSB, dfSSB = # of groups - 1 = I - 1**\n",
    "- **the degrees of freedom for SSW, dfSSW = N - I**\n",
    "- then we perform a hypothesis test to determine if the group means are equal! \n",
    "    - $H_0$: $\\mu_1 = \\mu_2 = \\mu_3 = \\mu_I$\n",
    "    - $H_1$: $\\mu_i \\ne \\mu_j$ for some $(i, j)$. \n",
    "- our test is: F = $\\frac{\\frac{SSB}{dfSSB}}{\\frac{SSW}{dfSSW}}$\n",
    "- our rejection region is: F $\\ge$ $F_{(\\alpha,I-1,N-1)}$ where I is number of groups and N in number of data points in total in 3 groups. \n",
    "\n",
    "# The ANOVA Table (On exam)\n",
    "\n",
    "- | **Anova**         |       **SS**    |-------**Degrees of Freedom (DF)------------ **| **SS / DF** |\n",
    "-------------------------------------------------------------------------------\n",
    "- | **SSB:**| $\\Sigma_{i=1}^{I}\\Sigma_{j=1}^{n}(\\bar{Y_i} - \\bar{\\bar{Y}})^2$ | I - 1 | $\\frac{\\Sigma_{i=1}^{I}\\Sigma_{j=1}^{n}(\\bar{Y_i} - \\bar{\\bar{Y}})^2}{I-1}$\n",
    "- | **SSW:**| $\\Sigma_{i=1}^{I}\\Sigma_{j=1}^{n}(y_{i,j} - \\bar{Y_i})^2$ | N - I | $\\frac{\\Sigma_{i=1}^{I}\\Sigma_{j=1}^{n}(y_{i,j} - \\bar{Y_i})^2}{N - I}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "- instead of using regression to predict values of the response, we can use regression to classify data; binary classification of data based on features and a now the response is converted to 0 or 1 being 1 of two output classes.\n",
    "- model is $y = B_0 + B_1(X)$, then we use the sigmoid function:\n",
    "- $P(y=1 \\mid x) = \\frac{1}{1 + e^{-(B_0 + B_1(X))} }$ = the probability that the data point x is of class type 1.\n",
    "\n",
    "\n",
    "# Odds:\n",
    "- if something has a p of 0.75 of occuring the odds are: 0.75:0.25 = 3:1 = the ratio of p/(1-p) is the odds.\n",
    "\n",
    "\n",
    "\n",
    "# Iterative Solutions:\n",
    "- iterative solutions means in general a strategy of solving problems where we:\n",
    "    - make a guess at the parameter $X^0$\n",
    "    - update the guess in a smart way based on problems specifics. \n",
    "    - repeat in a way that continues until you converge on the correct answer.\n",
    "- if a function is convex up (2nd derivative is positive everywhere ($x^2$ becomes 2x becomes 2.) then we can find the minimum using an iterative gradient descent even for MLR with 2 variables!\n",
    "- procedure for minimization of $F(z)$ = $z^2 - 2z + 2$ using gradient descent:\n",
    "    - make some initial guess of the z corresponding to the minimum value, $z^{(0)}$\n",
    "    - continue, making a smart improvement in each step, continue from $z^{(0)}$ to $z^{(1)}$ to $z^{(k)}$ where the kth value of z reaches some stopping condition. \n",
    "    - the formula to follow for gradient descent:\n",
    "        - $z^{(k+1)}$ = $z^{(k)}$ - $\\mu(f'(z^{(k)})$ where $\\mu$ is the learning rate, a constant that you multiply by the derivative, \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using gradient descent for SSE of MLR (2 vars!)\n",
    "- take partial derivatives and do the same process but in the direction of the partial that corresponds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
