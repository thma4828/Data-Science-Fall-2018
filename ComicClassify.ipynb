{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComicDataDC = pd.read_csv(\"C:/Users/tsmar/dc-wikia-data.csv\")\n",
    "ComicDataMarvel = pd.read_csv(\"C:/Users/tsmar/marvel-wikia-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComicDataDC.fillna(value=0, axis=0, inplace=True)\n",
    "ComicDataMarvel.fillna(value=0, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComicData_DC = ComicDataDC.drop(labels=['urlslug', 'page_id', 'GSM', 'FIRST APPEARANCE', 'ALIGN'], axis=1) #drop unwanted cols\n",
    "ComicData_Marvel = ComicDataMarvel.drop(labels=['urlslug', 'page_id', 'GSM', 'FIRST APPEARANCE', 'ALIGN'], axis=1)\n",
    "\n",
    "#ComicData3 = ComicData2.fillna(0, axis=0) #drop rows with NaN values.\n",
    "Target_DC = ComicDataDC.iloc[:, 4]\n",
    "Target_Marvel = ComicDataMarvel.iloc[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16376"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Target_DC)\n",
    "len(Target_Marvel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will be prediction the alignment of a superhero based on other data about them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, this dataframe needs to be converted into numerical data so that machine learning can be run on it. \n",
    "#lets build some dictionaries!!!!\n",
    "eye_map = {'Blue Eyes' : 1, 'Green Eyes' : 2, 'Brown Eyes' : 3, 'Black Eyes' : 4, 'Red Eyes' : 5, 'White Eyes' : 6,\n",
    "           'Hazel Eyes' : 7,  'Yellow Eyes' : 8, 'Purple Eyes' : 9, 'Orange Eyes' : 10, 'Pink Eyes' : 11, 'Silver Eyes' : 12,\n",
    "           'Gold Eyes' : 13, 'Grey Eyes' : 14, 'Photocellular Eyes' : 15, 'Amber Eyes' : 16, 'Violet Eyes':17, 'Auburn Hair':20, 'Black Eyeballs': 4, \n",
    "           0:0, 'Variable Eyes':21, 'One Eye':22, 'Multiple Eyes' : 23, 'Magenta Eyes':17, 'Yellow Eyeballs' : 8, 'No Eyes':24,\n",
    "          'Compound Eyes' : 25}\n",
    "\n",
    "hair_map = {'Blue Hair' : 1, 'Green Hair' : 2, 'Brown Hair' : 3, 'Black Hair' : 4, 'Red Hair' : 5, 'White Hair' : 6,\n",
    "           'Hazel Hair' : 7,  'Yellow Hair' : 8, 'Purple Hair' : 9, 'Orange Hair' : 10, 'Pink Hair' : 11, 'Silver Hair' : 12, \n",
    "            'Blond Hair' : 13, 'Strawberry Blond Hair' : 14, 'Gold Hair' : 15, 'Grey Hair' : 16, 'Reddish Brown Hair':17, 'Violet Hair':18,\n",
    "            'Platinum Blond Hair':19, 'Auburn Hair':20, 0:0, 'No Hair':21, 'Bald' : 21, 'Reddish Blond Hair' : 14,\n",
    "           'Variable Hair': 23, 'Light Brown Hair' : 3, 'Magenta Hair' : 9, 'Bronze Hair' : 24, 'Dyed Hair' : 23,\n",
    "           'Orange-brown Hair': 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = {'Secret Identity' : 0, 'Public Identity' : 1, 'Identity Unknown': 2, 'No Dual Identity':3, 'Known to Authorities Identity':4, \n",
    "          0:0}\n",
    "gender_map = {'Male Characters' : 0, 'Female Characters' : 1, 'Genderless Characters': 2, 'Transgender Characters': 3,'Genderfluid Characters': 4, 'Agender Characters':5, 0:0}\n",
    "alive_map = {'Living Characters' : 0, 'Deceased Characters' : 1, 0:0}\n",
    "align_map = {'Good Characters' : 0, 'Neutral Characters' : 1, 'Bad Characters' : 2, 'Reformed Criminals' : 3, 0:4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will predict if a character is aligned to Good or Evil or Reformed based on these following data items:\n",
    "    #1. Gender,\n",
    "    #2. Eye Color\n",
    "    #3. is Alive\n",
    "    #4. Number of Appearences\n",
    "    #5. Year\n",
    "    #6. simple hash of their name.\n",
    "    #7. hair color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                      ALIVE  APPEARANCES         EYE        HAIR  \\\n",
       "0        Living Characters       3093.0   Blue Eyes  Black Hair   \n",
       "1        Living Characters       2496.0   Blue Eyes  Black Hair   \n",
       "2        Living Characters       1565.0  Brown Eyes  Brown Hair   \n",
       "3        Living Characters       1316.0  Brown Eyes  White Hair   \n",
       "4        Living Characters       1237.0   Blue Eyes  Black Hair   \n",
       "5        Living Characters       1231.0   Blue Eyes  Black Hair   \n",
       "6        Living Characters       1121.0   Blue Eyes  Blond Hair   \n",
       "7        Living Characters       1095.0   Blue Eyes  Black Hair   \n",
       "8        Living Characters       1075.0   Blue Eyes  Blond Hair   \n",
       "9        Living Characters       1028.0   Blue Eyes  Blond Hair   \n",
       "10       Living Characters       1028.0   Blue Eyes  Blond Hair   \n",
       "11     Deceased Characters        969.0   Blue Eyes  Blond Hair   \n",
       "12       Living Characters        951.0   Blue Eyes    Red Hair   \n",
       "13       Living Characters        951.0   Blue Eyes  Brown Hair   \n",
       "14       Living Characters        934.0   Blue Eyes  Black Hair   \n",
       "15       Living Characters        930.0   Blue Eyes  Black Hair   \n",
       "16       Living Characters        803.0   Blue Eyes  Brown Hair   \n",
       "17       Living Characters        716.0  Green Eyes  Black Hair   \n",
       "18       Living Characters        706.0  Brown Eyes           0   \n",
       "19       Living Characters        677.0  Green Eyes           0   \n",
       "20       Living Characters        654.0  Green Eyes    Red Hair   \n",
       "21       Living Characters        635.0   Blue Eyes  Blond Hair   \n",
       "22       Living Characters        605.0   Blue Eyes  Black Hair   \n",
       "23       Living Characters        595.0  Green Eyes  Green Hair   \n",
       "24       Living Characters        593.0   Blue Eyes    Red Hair   \n",
       "25       Living Characters        584.0  Brown Eyes  Black Hair   \n",
       "26       Living Characters        560.0   Blue Eyes  Black Hair   \n",
       "27     Deceased Characters        558.0   Blue Eyes    Red Hair   \n",
       "28       Living Characters        557.0  Green Eyes    Red Hair   \n",
       "29       Living Characters        549.0  Brown Eyes  Black Hair   \n",
       "...                    ...          ...         ...         ...   \n",
       "23242  Deceased Characters          0.0           0           0   \n",
       "23243                    0          0.0           0           0   \n",
       "23244    Living Characters          0.0           0     No Hair   \n",
       "23245    Living Characters          0.0  White Eyes           0   \n",
       "23246    Living Characters          0.0           0           0   \n",
       "23247    Living Characters          0.0           0  Black Hair   \n",
       "23248    Living Characters          0.0           0           0   \n",
       "23249    Living Characters          0.0           0           0   \n",
       "23250  Deceased Characters          0.0           0           0   \n",
       "23251    Living Characters          0.0           0           0   \n",
       "23252    Living Characters          0.0           0     No Hair   \n",
       "23253    Living Characters          0.0           0           0   \n",
       "23254    Living Characters          0.0           0  Blond Hair   \n",
       "23255    Living Characters          0.0  Black Eyes     No Hair   \n",
       "23256    Living Characters          0.0  Black Eyes   Grey Hair   \n",
       "23257  Deceased Characters          0.0           0           0   \n",
       "23258    Living Characters          0.0    Red Eyes           0   \n",
       "23259    Living Characters          0.0  Black Eyes        Bald   \n",
       "23260    Living Characters          0.0  Hazel Eyes        Bald   \n",
       "23261  Deceased Characters          0.0           0           0   \n",
       "23262    Living Characters          0.0  Brown Eyes  Black Hair   \n",
       "23263    Living Characters          0.0  Hazel Eyes        Bald   \n",
       "23264    Living Characters          0.0           0  Brown Hair   \n",
       "23265    Living Characters          0.0   Blue Eyes  Black Hair   \n",
       "23266    Living Characters          0.0           0           0   \n",
       "23267    Living Characters          0.0  Green Eyes     No Hair   \n",
       "23268    Living Characters          0.0   Blue Eyes        Bald   \n",
       "23269    Living Characters          0.0  Black Eyes        Bald   \n",
       "23270    Living Characters          0.0           0           0   \n",
       "23271    Living Characters          0.0           0           0   \n",
       "\n",
       "                     ID                SEX    YEAR  Year  \\\n",
       "0       Secret Identity    Male Characters  1939.0   NaN   \n",
       "1       Secret Identity    Male Characters  1986.0   NaN   \n",
       "2       Secret Identity    Male Characters  1959.0   NaN   \n",
       "3       Public Identity    Male Characters  1987.0   NaN   \n",
       "4       Secret Identity    Male Characters  1940.0   NaN   \n",
       "5       Public Identity  Female Characters  1941.0   NaN   \n",
       "6       Public Identity    Male Characters  1941.0   NaN   \n",
       "7       Secret Identity    Male Characters  1989.0   NaN   \n",
       "8       Public Identity  Female Characters  1969.0   NaN   \n",
       "9       Secret Identity    Male Characters  1956.0   NaN   \n",
       "10      Secret Identity  Female Characters  1956.0   NaN   \n",
       "11      Secret Identity    Male Characters  1940.0   NaN   \n",
       "12      Secret Identity  Female Characters  1967.0   NaN   \n",
       "13      Public Identity    Male Characters  1940.0   NaN   \n",
       "14      Public Identity  Female Characters  1938.0   NaN   \n",
       "15      Public Identity    Male Characters  1943.0   NaN   \n",
       "16      Secret Identity    Male Characters  1940.0   NaN   \n",
       "17      Secret Identity    Male Characters  1994.0   NaN   \n",
       "18      Public Identity    Male Characters  1961.0   NaN   \n",
       "19      Public Identity    Male Characters  1986.0   NaN   \n",
       "20      Secret Identity    Male Characters  1941.0   NaN   \n",
       "21      Secret Identity  Female Characters  1976.0   NaN   \n",
       "22      Secret Identity    Male Characters  1942.0   NaN   \n",
       "23      Public Identity    Male Characters  1965.0   NaN   \n",
       "24      Public Identity    Male Characters  1968.0   NaN   \n",
       "25      Public Identity    Male Characters  1980.0   NaN   \n",
       "26      Secret Identity    Male Characters  1993.0   NaN   \n",
       "27      Public Identity    Male Characters  1960.0   NaN   \n",
       "28      Public Identity    Male Characters  1986.0   NaN   \n",
       "29      Public Identity    Male Characters  1971.0   NaN   \n",
       "...                 ...                ...     ...   ...   \n",
       "23242                 0    Male Characters     NaN   0.0   \n",
       "23243                 0                  0     NaN   0.0   \n",
       "23244   Secret Identity                  0     NaN   0.0   \n",
       "23245   Public Identity    Male Characters     NaN   0.0   \n",
       "23246                 0                  0     NaN   0.0   \n",
       "23247  No Dual Identity    Male Characters     NaN   0.0   \n",
       "23248                 0    Male Characters     NaN   0.0   \n",
       "23249                 0    Male Characters     NaN   0.0   \n",
       "23250                 0    Male Characters     NaN   0.0   \n",
       "23251   Public Identity    Male Characters     NaN   0.0   \n",
       "23252   Public Identity    Male Characters     NaN   0.0   \n",
       "23253   Secret Identity    Male Characters     NaN   0.0   \n",
       "23254   Secret Identity  Female Characters     NaN   0.0   \n",
       "23255  No Dual Identity    Male Characters     NaN   0.0   \n",
       "23256  No Dual Identity  Female Characters     NaN   0.0   \n",
       "23257  No Dual Identity  Female Characters     NaN   0.0   \n",
       "23258   Public Identity  Female Characters     NaN   0.0   \n",
       "23259   Public Identity    Male Characters     NaN   0.0   \n",
       "23260   Secret Identity    Male Characters     NaN   0.0   \n",
       "23261                 0    Male Characters     NaN   0.0   \n",
       "23262   Public Identity  Female Characters     NaN   0.0   \n",
       "23263   Public Identity    Male Characters     NaN   0.0   \n",
       "23264   Public Identity  Female Characters     NaN   0.0   \n",
       "23265   Public Identity  Female Characters     NaN   0.0   \n",
       "23266                 0                  0     NaN   0.0   \n",
       "23267  No Dual Identity    Male Characters     NaN   0.0   \n",
       "23268  No Dual Identity    Male Characters     NaN   0.0   \n",
       "23269   Secret Identity    Male Characters     NaN   0.0   \n",
       "23270   Secret Identity    Male Characters     NaN   0.0   \n",
       "23271                 0                  0     NaN   0.0   \n",
       "\n",
       "                                                    name  \n",
       "0                                   Batman (Bruce Wayne)  \n",
       "1                                  Superman (Clark Kent)  \n",
       "2                             Green Lantern (Hal Jordan)  \n",
       "3                               James Gordon (New Earth)  \n",
       "4                            Richard Grayson (New Earth)  \n",
       "5                            Wonder Woman (Diana Prince)  \n",
       "6                                 Aquaman (Arthur Curry)  \n",
       "7                              Timothy Drake (New Earth)  \n",
       "8                         Dinah Laurel Lance (New Earth)  \n",
       "9                                    Flash (Barry Allen)  \n",
       "10                                            GenderTest  \n",
       "11                                Alan Scott (New Earth)  \n",
       "12                            Barbara Gordon (New Earth)  \n",
       "13                             Jason Garrick (New Earth)  \n",
       "14                                 Lois Lane (New Earth)  \n",
       "15                         Alfred Pennyworth (New Earth)  \n",
       "16                               Carter Hall (New Earth)  \n",
       "17                               Kyle Rayner (New Earth)  \n",
       "18                            Raymond Palmer (New Earth)  \n",
       "19                          Alexander Luthor (New Earth)  \n",
       "20                                Roy Harper (New Earth)  \n",
       "21                                Kara Zor-L (Earth-Two)  \n",
       "22                                 Ted Grant (New Earth)  \n",
       "23                            Garfield Logan (New Earth)  \n",
       "24                               Guy Gardner (New Earth)  \n",
       "25                              Victor Stone (New Earth)  \n",
       "26                                    Kon-El (New Earth)  \n",
       "27                               Ralph Dibny (New Earth)  \n",
       "28                               James Olsen (New Earth)  \n",
       "29                              John Stewart (New Earth)  \n",
       "...                                                  ...  \n",
       "23242                                Thiazzi (Earth-616)  \n",
       "23243                                          TOR\\/test  \n",
       "23244                       Toxin (Luminals) (Earth-616)  \n",
       "23245                          Urizen Ul'var (Earth-616)  \n",
       "23246                                  Valka (Earth-616)  \n",
       "23247                                  Vayor (Earth-616)  \n",
       "23248                               Viridian (Earth-616)  \n",
       "23249                          William Burke (Earth-616)  \n",
       "23250                      William Falsworth (Earth-616)  \n",
       "23251                    William Shakespeare (Earth-616)  \n",
       "23252                       Yamata no Orichi (Earth-616)  \n",
       "23253  Zero G. Priestly (Legion Personality) (Earth-616)  \n",
       "23254                            Zora Loftus (Earth-616)  \n",
       "23255                                   Agar (Earth-616)  \n",
       "23256      Ana (Natasha Romanoff's neighbor) (Earth-616)  \n",
       "23257                         Dante's mother (Earth-616)  \n",
       "23258                               Farbauti (Earth-616)  \n",
       "23259         Finch (Kate Bishop's neighbor) (Earth-616)  \n",
       "23260              Jack O'Lantern (Impostor) (Earth-616)  \n",
       "23261                                 K'thol (Earth-616)  \n",
       "23262            Karen (Hijack's girlfriend) (Earth-616)  \n",
       "23263        Marcus (Kate Bishop's neighbor) (Earth-616)  \n",
       "23264               Marcy (Offer's employee) (Earth-616)  \n",
       "23265                         Melanie Kapoor (Earth-616)  \n",
       "23266                       Phoenix's Shadow (Earth-616)  \n",
       "23267                                 Ru'ach (Earth-616)  \n",
       "23268                    Thane (Thanos' son) (Earth-616)  \n",
       "23269                      Tinkerer (Skrull) (Earth-616)  \n",
       "23270                     TK421 (Spiderling) (Earth-616)  \n",
       "23271                              Yologarch (Earth-616)  \n",
       "\n",
       "[23272 rows x 9 columns]>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ComicData = pd.concat([ComicData_DC, ComicData_Marvel], ignore_index=True, axis=0)\n",
    "ComicAllignment = pd.concat([Target_DC, Target_Marvel], ignore_index=True, axis=0)\n",
    "\n",
    "ComicAllignment\n",
    "ComicData.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we will turn list of alignments into list of numbers: \n",
    "import math\n",
    "num_align = [align_map[d] for d in ComicAllignment] #these are our labels. \n",
    "num_gender = [gender_map[g[5]] for g in ComicData.values]\n",
    "num_eye_color = [eye_map[c[2]] for c in ComicData.values]\n",
    "num_isAlive = [alive_map[a[0]] for a in ComicData.values]\n",
    "num_hair = [hair_map[h[3]] for h in ComicData.values]\n",
    "num_id = [id_map[p[4]] for p in ComicData.values]\n",
    "num_name = [len(word[8]) for word in ComicData.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Living Characters', 3093.0, 'Blue Eyes', 'Black Hair',\n",
       "       'Secret Identity', 'Male Characters', 1939.0, nan,\n",
       "       'Batman (Bruce Wayne)'], dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ComicData.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        ALIVE  APPEARANCES  EYE  HAIR  ID  SEX  name\n",
       "0          0       3093.0    1     4   0    0    20\n",
       "1          0       2496.0    1     4   0    0    21\n",
       "2          0       1565.0    3     3   0    0    26\n",
       "3          0       1316.0    3     6   1    0    24\n",
       "4          0       1237.0    1     4   0    0    27\n",
       "5          0       1231.0    1     4   1    1    27\n",
       "6          0       1121.0    1    13   1    0    22\n",
       "7          0       1095.0    1     4   0    0    25\n",
       "8          0       1075.0    1    13   1    1    30\n",
       "9          0       1028.0    1    13   0    0    19\n",
       "10         0       1028.0    1    13   0    1    10\n",
       "11         1        969.0    1    13   0    0    22\n",
       "12         0        951.0    1     5   0    1    26\n",
       "13         0        951.0    1     3   1    0    25\n",
       "14         0        934.0    1     4   1    1    21\n",
       "15         0        930.0    1     4   1    0    29\n",
       "16         0        803.0    1     3   0    0    23\n",
       "17         0        716.0    2     4   0    0    23\n",
       "18         0        706.0    3     0   1    0    26\n",
       "19         0        677.0    2     0   1    0    28\n",
       "20         0        654.0    2     5   0    0    22\n",
       "21         0        635.0    1    13   0    1    22\n",
       "22         0        605.0    1     4   0    0    21\n",
       "23         0        595.0    2     2   1    0    26\n",
       "24         0        593.0    1     5   1    0    23\n",
       "25         0        584.0    3     4   1    0    24\n",
       "26         0        560.0    1     4   0    0    18\n",
       "27         1        558.0    1     5   1    0    23\n",
       "28         0        557.0    2     5   1    0    23\n",
       "29         0        549.0    3     4   1    0    24\n",
       "...      ...          ...  ...   ...  ..  ...   ...\n",
       "23242      1          0.0    0     0   0    0    19\n",
       "23243      0          0.0    0     0   0    0     9\n",
       "23244      0          0.0    0    21   0    0    28\n",
       "23245      0          0.0    6     0   1    0    25\n",
       "23246      0          0.0    0     0   0    0    17\n",
       "23247      0          0.0    0     4   3    0    17\n",
       "23248      0          0.0    0     0   0    0    20\n",
       "23249      0          0.0    0     0   0    0    25\n",
       "23250      1          0.0    0     0   0    0    29\n",
       "23251      0          0.0    0     0   1    0    31\n",
       "23252      0          0.0    0    21   1    0    28\n",
       "23253      0          0.0    0     0   0    0    49\n",
       "23254      0          0.0    0    13   0    1    23\n",
       "23255      0          0.0    4    21   3    0    16\n",
       "23256      0          0.0    4    16   3    1    45\n",
       "23257      1          0.0    0     0   3    1    26\n",
       "23258      0          0.0    5     0   1    1    20\n",
       "23259      0          0.0    4    21   1    0    42\n",
       "23260      0          0.0    7    21   0    0    37\n",
       "23261      1          0.0    0     0   0    0    18\n",
       "23262      0          0.0    3     4   1    1    39\n",
       "23263      0          0.0    7    21   1    0    43\n",
       "23264      0          0.0    0     3   1    1    36\n",
       "23265      0          0.0    1     4   1    1    26\n",
       "23266      0          0.0    0     0   0    0    28\n",
       "23267      0          0.0    2    21   3    0    18\n",
       "23268      0          0.0    1    21   3    0    31\n",
       "23269      0          0.0    4    21   0    0    29\n",
       "23270      0          0.0    0     0   0    0    30\n",
       "23271      0          0.0    0     0   0    0    21\n",
       "\n",
       "[23272 rows x 7 columns]>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ComicData['ID'] = num_id\n",
    "ComicData['EYE'] = num_eye_color\n",
    "ComicData['ALIVE'] = num_isAlive\n",
    "ComicData['HAIR'] = num_hair\n",
    "ComicData['SEX'] = num_gender\n",
    "ComicData['name'] = num_name\n",
    "ComicData.drop(labels=['Year', 'YEAR'], axis=1, inplace=True)\n",
    "ComicData.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        ALIVE  APPEARANCES  EYE  HAIR  ID  SEX  name\n",
       "0          0       3093.0    1     4   0    0    20\n",
       "1          0       2496.0    1     4   0    0    21\n",
       "2          0       1565.0    3     3   0    0    26\n",
       "3          0       1316.0    3     6   1    0    24\n",
       "4          0       1237.0    1     4   0    0    27\n",
       "5          0       1231.0    1     4   1    1    27\n",
       "6          0       1121.0    1    13   1    0    22\n",
       "7          0       1095.0    1     4   0    0    25\n",
       "8          0       1075.0    1    13   1    1    30\n",
       "9          0       1028.0    1    13   0    0    19\n",
       "10         0       1028.0    1    13   0    1    10\n",
       "11         1        969.0    1    13   0    0    22\n",
       "12         0        951.0    1     5   0    1    26\n",
       "13         0        951.0    1     3   1    0    25\n",
       "14         0        934.0    1     4   1    1    21\n",
       "15         0        930.0    1     4   1    0    29\n",
       "16         0        803.0    1     3   0    0    23\n",
       "17         0        716.0    2     4   0    0    23\n",
       "18         0        706.0    3     0   1    0    26\n",
       "19         0        677.0    2     0   1    0    28\n",
       "20         0        654.0    2     5   0    0    22\n",
       "21         0        635.0    1    13   0    1    22\n",
       "22         0        605.0    1     4   0    0    21\n",
       "23         0        595.0    2     2   1    0    26\n",
       "24         0        593.0    1     5   1    0    23\n",
       "25         0        584.0    3     4   1    0    24\n",
       "26         0        560.0    1     4   0    0    18\n",
       "27         1        558.0    1     5   1    0    23\n",
       "28         0        557.0    2     5   1    0    23\n",
       "29         0        549.0    3     4   1    0    24\n",
       "...      ...          ...  ...   ...  ..  ...   ...\n",
       "23242      1          0.0    0     0   0    0    19\n",
       "23243      0          0.0    0     0   0    0     9\n",
       "23244      0          0.0    0    21   0    0    28\n",
       "23245      0          0.0    6     0   1    0    25\n",
       "23246      0          0.0    0     0   0    0    17\n",
       "23247      0          0.0    0     4   3    0    17\n",
       "23248      0          0.0    0     0   0    0    20\n",
       "23249      0          0.0    0     0   0    0    25\n",
       "23250      1          0.0    0     0   0    0    29\n",
       "23251      0          0.0    0     0   1    0    31\n",
       "23252      0          0.0    0    21   1    0    28\n",
       "23253      0          0.0    0     0   0    0    49\n",
       "23254      0          0.0    0    13   0    1    23\n",
       "23255      0          0.0    4    21   3    0    16\n",
       "23256      0          0.0    4    16   3    1    45\n",
       "23257      1          0.0    0     0   3    1    26\n",
       "23258      0          0.0    5     0   1    1    20\n",
       "23259      0          0.0    4    21   1    0    42\n",
       "23260      0          0.0    7    21   0    0    37\n",
       "23261      1          0.0    0     0   0    0    18\n",
       "23262      0          0.0    3     4   1    1    39\n",
       "23263      0          0.0    7    21   1    0    43\n",
       "23264      0          0.0    0     3   1    1    36\n",
       "23265      0          0.0    1     4   1    1    26\n",
       "23266      0          0.0    0     0   0    0    28\n",
       "23267      0          0.0    2    21   3    0    18\n",
       "23268      0          0.0    1    21   3    0    31\n",
       "23269      0          0.0    4    21   0    0    29\n",
       "23270      0          0.0    0     0   0    0    30\n",
       "23271      0          0.0    0     0   0    0    21\n",
       "\n",
       "[23272 rows x 7 columns]>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ComicData.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23272"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so we have our labels: \n",
    "len(num_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the data is ready! we can go to work!\n",
    "#lets split up the data into training, validation, testing\n",
    "#now i will also add normalization!\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "TrainSplit = 5/8\n",
    "ValidSplit = 1/8\n",
    "TestSplit = 2/8\n",
    "\n",
    "data_size = 23272\n",
    "train_len = int(data_size * TrainSplit)\n",
    "valid_len = int(data_size * ValidSplit)\n",
    "test_len = int(data_size * TestSplit)\n",
    "\n",
    "TrainSet = mms.fit_transform(ComicData.values[:train_len])\n",
    "TrainLabels = num_align[:train_len]\n",
    "\n",
    "ValidationSet = mms.transform(ComicData.values[train_len+1:train_len+valid_len+1])\n",
    "ValidationLabels = num_align[train_len+1:train_len+valid_len+1]\n",
    "\n",
    "TestSet = mms.transform(ComicData.values[train_len + valid_len:])\n",
    "TestLabels = num_align[train_len + valid_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=7, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=7, activation='relu', init='uniform'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(64, activation='relu', init='uniform'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(64, activation='relu', init='uniform'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(32, activation='relu', init='uniform'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(16, activation='relu', init='uniform'))\n",
    "model.add(Dense(5, activation='softmax', init='uniform'))\n",
    "model.compile(optimizer=Adam(lr=0.0003),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14545"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TrainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet = np.reshape(TrainSet, (train_len, 7))\n",
    "TrainLabelSet = []\n",
    "for t in TrainLabels:\n",
    "    if t == 0:\n",
    "        TrainLabelSet.append([1, 0, 0, 0, 0])\n",
    "    elif t == 1:\n",
    "        TrainLabelSet.append([0, 1, 0, 0, 0])\n",
    "    elif t == 2:\n",
    "        TrainLabelSet.append([0, 0, 1, 0, 0])\n",
    "    elif t == 3:\n",
    "        TrainLabelSet.append([0, 0, 0, 1, 0])\n",
    "    elif t == 4:\n",
    "        TrainLabelSet.append([0, 0, 0, 0, 1])\n",
    "ValidationSet = np.reshape(ValidationSet, (valid_len, 7))\n",
    "ValidLabelSet = []\n",
    "for t in ValidationLabels:\n",
    "    if t == 0:\n",
    "        ValidLabelSet.append([1, 0, 0, 0, 0])\n",
    "    elif t == 1:\n",
    "        ValidLabelSet.append([0, 1, 0, 0, 0])\n",
    "    elif t == 2:\n",
    "        ValidLabelSet.append([0, 0, 1, 0, 0])\n",
    "    elif t == 3:\n",
    "        ValidLabelSet.append([0, 0, 0, 1, 0])\n",
    "    elif t == 4:\n",
    "        ValidLabelSet.append([0, 0, 0, 0, 1])\n",
    "        \n",
    "TestSet = np.reshape(TestSet, (test_len, 7))\n",
    "TestLabelSet = []\n",
    "for t in TestLabels:\n",
    "    if t == 0:\n",
    "        TestLabelSet.append([1, 0, 0, 0, 0])\n",
    "    elif t == 1:\n",
    "        TestLabelSet.append([0, 1, 0, 0, 0])\n",
    "    elif t == 2:\n",
    "        TestLabelSet.append([0, 0, 1, 0, 0])\n",
    "    elif t == 3:\n",
    "        TestLabelSet.append([0, 0, 0, 1, 0])\n",
    "    elif t == 4:\n",
    "        TestLabelSet.append([0, 0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14545/14545 [==============================] - 1s 95us/step - loss: 1.3238 - acc: 0.3904\n",
      "Epoch 2/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.2285 - acc: 0.3914\n",
      "Epoch 3/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.2261 - acc: 0.3950\n",
      "Epoch 4/100\n",
      "14545/14545 [==============================] - 1s 55us/step - loss: 1.2252 - acc: 0.3994\n",
      "Epoch 5/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.2238 - acc: 0.4120\n",
      "Epoch 6/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.2215 - acc: 0.4183\n",
      "Epoch 7/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.2193 - acc: 0.4374\n",
      "Epoch 8/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.2182 - acc: 0.4504\n",
      "Epoch 9/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.2146 - acc: 0.4628\n",
      "Epoch 10/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.2106 - acc: 0.4717\n",
      "Epoch 11/100\n",
      "14545/14545 [==============================] - 1s 64us/step - loss: 1.2066 - acc: 0.4821\n",
      "Epoch 12/100\n",
      "14545/14545 [==============================] - 1s 57us/step - loss: 1.2015 - acc: 0.4890\n",
      "Epoch 13/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.1934 - acc: 0.4919\n",
      "Epoch 14/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.1885 - acc: 0.4942\n",
      "Epoch 15/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1853 - acc: 0.4966\n",
      "Epoch 16/100\n",
      "14545/14545 [==============================] - 1s 62us/step - loss: 1.1808 - acc: 0.4983\n",
      "Epoch 17/100\n",
      "14545/14545 [==============================] - 1s 69us/step - loss: 1.1809 - acc: 0.4988\n",
      "Epoch 18/100\n",
      "14545/14545 [==============================] - 1s 62us/step - loss: 1.1772 - acc: 0.5027: 1s - loss: 1.1\n",
      "Epoch 19/100\n",
      "14545/14545 [==============================] - 1s 57us/step - loss: 1.1764 - acc: 0.5021\n",
      "Epoch 20/100\n",
      "14545/14545 [==============================] - 1s 57us/step - loss: 1.1739 - acc: 0.5029\n",
      "Epoch 21/100\n",
      "14545/14545 [==============================] - 1s 61us/step - loss: 1.1731 - acc: 0.5056\n",
      "Epoch 22/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.1720 - acc: 0.5052\n",
      "Epoch 23/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.1718 - acc: 0.5055\n",
      "Epoch 24/100\n",
      "14545/14545 [==============================] - 1s 57us/step - loss: 1.1715 - acc: 0.5050\n",
      "Epoch 25/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1707 - acc: 0.5083\n",
      "Epoch 26/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1692 - acc: 0.5069\n",
      "Epoch 27/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1708 - acc: 0.5064\n",
      "Epoch 28/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1698 - acc: 0.5076\n",
      "Epoch 29/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1684 - acc: 0.5066\n",
      "Epoch 30/100\n",
      "14545/14545 [==============================] - 1s 60us/step - loss: 1.1682 - acc: 0.5057\n",
      "Epoch 31/100\n",
      "14545/14545 [==============================] - 1s 61us/step - loss: 1.1673 - acc: 0.5080\n",
      "Epoch 32/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1671 - acc: 0.5073\n",
      "Epoch 33/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1667 - acc: 0.5088\n",
      "Epoch 34/100\n",
      "14545/14545 [==============================] - 1s 75us/step - loss: 1.1650 - acc: 0.5104\n",
      "Epoch 35/100\n",
      "14545/14545 [==============================] - 1s 72us/step - loss: 1.1644 - acc: 0.5090\n",
      "Epoch 36/100\n",
      "14545/14545 [==============================] - 1s 61us/step - loss: 1.1647 - acc: 0.5123\n",
      "Epoch 37/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1639 - acc: 0.5068\n",
      "Epoch 38/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1623 - acc: 0.5108\n",
      "Epoch 39/100\n",
      "14545/14545 [==============================] - 1s 60us/step - loss: 1.1642 - acc: 0.5068\n",
      "Epoch 40/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1637 - acc: 0.5088\n",
      "Epoch 41/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1623 - acc: 0.5110\n",
      "Epoch 42/100\n",
      "14545/14545 [==============================] - 1s 62us/step - loss: 1.1636 - acc: 0.5084\n",
      "Epoch 43/100\n",
      "14545/14545 [==============================] - 1s 63us/step - loss: 1.1606 - acc: 0.5119\n",
      "Epoch 44/100\n",
      "14545/14545 [==============================] - 1s 57us/step - loss: 1.1601 - acc: 0.5132\n",
      "Epoch 45/100\n",
      "14545/14545 [==============================] - 1s 64us/step - loss: 1.1609 - acc: 0.5128\n",
      "Epoch 46/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1609 - acc: 0.5124\n",
      "Epoch 47/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1601 - acc: 0.5111\n",
      "Epoch 48/100\n",
      "14545/14545 [==============================] - 1s 57us/step - loss: 1.1595 - acc: 0.5141\n",
      "Epoch 49/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1593 - acc: 0.5134\n",
      "Epoch 50/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1579 - acc: 0.5143: 0s - loss: 1.1631 - acc\n",
      "Epoch 51/100\n",
      "14545/14545 [==============================] - 1s 57us/step - loss: 1.1587 - acc: 0.5136\n",
      "Epoch 52/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1580 - acc: 0.5150\n",
      "Epoch 53/100\n",
      "14545/14545 [==============================] - 1s 66us/step - loss: 1.1581 - acc: 0.5147\n",
      "Epoch 54/100\n",
      "14545/14545 [==============================] - 1s 66us/step - loss: 1.1581 - acc: 0.5166\n",
      "Epoch 55/100\n",
      "14545/14545 [==============================] - 1s 65us/step - loss: 1.1573 - acc: 0.5131\n",
      "Epoch 56/100\n",
      "14545/14545 [==============================] - 1s 72us/step - loss: 1.1588 - acc: 0.5126\n",
      "Epoch 57/100\n",
      "14545/14545 [==============================] - 1s 65us/step - loss: 1.1569 - acc: 0.5171: 1s - loss: 1.1774 - ETA: 0s - loss: 1.1556 - acc: 0.51\n",
      "Epoch 58/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1559 - acc: 0.5153\n",
      "Epoch 59/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1573 - acc: 0.5168\n",
      "Epoch 60/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1567 - acc: 0.5148\n",
      "Epoch 61/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1563 - acc: 0.5163\n",
      "Epoch 62/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1562 - acc: 0.5134\n",
      "Epoch 63/100\n",
      "14545/14545 [==============================] - 1s 63us/step - loss: 1.1553 - acc: 0.5150\n",
      "Epoch 64/100\n",
      "14545/14545 [==============================] - 1s 62us/step - loss: 1.1557 - acc: 0.5161\n",
      "Epoch 65/100\n",
      "14545/14545 [==============================] - 1s 62us/step - loss: 1.1550 - acc: 0.5136\n",
      "Epoch 66/100\n",
      "14545/14545 [==============================] - 1s 64us/step - loss: 1.1557 - acc: 0.5157\n",
      "Epoch 67/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1551 - acc: 0.5162\n",
      "Epoch 68/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1558 - acc: 0.5146\n",
      "Epoch 69/100\n",
      "14545/14545 [==============================] - 1s 60us/step - loss: 1.1543 - acc: 0.5174\n",
      "Epoch 70/100\n",
      "14545/14545 [==============================] - 1s 61us/step - loss: 1.1540 - acc: 0.5162\n",
      "Epoch 71/100\n",
      "14545/14545 [==============================] - 1s 73us/step - loss: 1.1538 - acc: 0.5153\n",
      "Epoch 72/100\n",
      "14545/14545 [==============================] - 1s 63us/step - loss: 1.1554 - acc: 0.5171\n",
      "Epoch 73/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1536 - acc: 0.5182\n",
      "Epoch 74/100\n",
      "14545/14545 [==============================] - 1s 62us/step - loss: 1.1540 - acc: 0.5202\n",
      "Epoch 75/100\n",
      "14545/14545 [==============================] - 1s 61us/step - loss: 1.1533 - acc: 0.5183\n",
      "Epoch 76/100\n",
      "14545/14545 [==============================] - 1s 62us/step - loss: 1.1534 - acc: 0.5183\n",
      "Epoch 77/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1546 - acc: 0.5152\n",
      "Epoch 78/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1539 - acc: 0.5189\n",
      "Epoch 79/100\n",
      "14545/14545 [==============================] - 1s 60us/step - loss: 1.1528 - acc: 0.5182\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14545/14545 [==============================] - 1s 62us/step - loss: 1.1528 - acc: 0.5184\n",
      "Epoch 81/100\n",
      "14545/14545 [==============================] - 1s 62us/step - loss: 1.1514 - acc: 0.5187\n",
      "Epoch 82/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1516 - acc: 0.5195\n",
      "Epoch 83/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.1532 - acc: 0.5167\n",
      "Epoch 84/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.1528 - acc: 0.5194\n",
      "Epoch 85/100\n",
      "14545/14545 [==============================] - 1s 55us/step - loss: 1.1520 - acc: 0.5185\n",
      "Epoch 86/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.1526 - acc: 0.5172\n",
      "Epoch 87/100\n",
      "14545/14545 [==============================] - 1s 57us/step - loss: 1.1510 - acc: 0.5181\n",
      "Epoch 88/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1509 - acc: 0.5191\n",
      "Epoch 89/100\n",
      "14545/14545 [==============================] - 1s 67us/step - loss: 1.1507 - acc: 0.5183\n",
      "Epoch 90/100\n",
      "14545/14545 [==============================] - 1s 65us/step - loss: 1.1513 - acc: 0.5160\n",
      "Epoch 91/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1516 - acc: 0.5125\n",
      "Epoch 92/100\n",
      "14545/14545 [==============================] - 1s 57us/step - loss: 1.1522 - acc: 0.5151\n",
      "Epoch 93/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1507 - acc: 0.5204\n",
      "Epoch 94/100\n",
      "14545/14545 [==============================] - 1s 60us/step - loss: 1.1513 - acc: 0.5186\n",
      "Epoch 95/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.1522 - acc: 0.5147\n",
      "Epoch 96/100\n",
      "14545/14545 [==============================] - 1s 56us/step - loss: 1.1516 - acc: 0.5169\n",
      "Epoch 97/100\n",
      "14545/14545 [==============================] - 1s 58us/step - loss: 1.1499 - acc: 0.5167\n",
      "Epoch 98/100\n",
      "14545/14545 [==============================] - 1s 62us/step - loss: 1.1506 - acc: 0.5183\n",
      "Epoch 99/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1499 - acc: 0.5169\n",
      "Epoch 100/100\n",
      "14545/14545 [==============================] - 1s 59us/step - loss: 1.1502 - acc: 0.5177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2364f1fa2e8>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(TrainSet, np.array(TrainLabelSet), epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2909/2909 [==============================] - 0s 24us/step\n",
      "\n",
      "acc: 45.69%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(ValidationSet, np.array(ValidLabelSet))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5818/5818 [==============================] - 0s 23us/step\n",
      "\n",
      "acc: 44.33%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(TestSet, np.array(TestLabelSet))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets try a randomforest\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=1, n_jobs=2)\n",
    "forest.fit(TrainSet, TrainLabelSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = forest.predict(TestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_match(yh, ya):\n",
    "    for i in range(len(yh)):\n",
    "        if yh[i] - ya[i] != 0.0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [label_match(yhat[i], np.array(TestLabelSet[i])) for i in range(len(yhat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3169474046063939"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(res) / len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets see about a random forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(criterion='entropy', n_estimators=1000, random_state=1, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=2,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(TrainSet, np.array(TrainLabelSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = forest.predict(TestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31866620831901"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(ValidationSet, np.array(ValidLabelSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Results:\n",
    "- my neural net does about 44% - 45% \n",
    "- even with 1000 decision trees I can only get 31% out of a random forest. \n",
    "- neural nets were better suited for a more sophisticated task such as comic character classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about a Support Vector Machine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as svm\n",
    "from sklearn.svm import SVC\n",
    "clf = svm.SVC(gamma='scale', decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14545"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TrainLabelSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14545, 5)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(TrainLabelSet).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about Linear Regression? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
